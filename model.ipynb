{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_palette(\"Paired\")"
      ],
      "metadata": {
        "id": "aEq-y_1XO9-Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/KODEX200.csv').loc[45:]"
      ],
      "metadata": {
        "id": "ek0UPu88UuUM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB6EokcGcTj4",
        "outputId": "06f54bd3-7bd4-46fc-929d-652f04802c16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum()) # 결측치 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27ui01UlVdqR",
        "outputId": "fe172b24-9ce7-410d-877a-27f8d8e7a70f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unnamed: 0      0\n",
            "date            0\n",
            "open            0\n",
            "high            0\n",
            "low             0\n",
            "close           0\n",
            "volume          0\n",
            "trade_volume    0\n",
            "ma              0\n",
            "macd            0\n",
            "macdsignal      0\n",
            "macdhist        0\n",
            "rsi             0\n",
            "ad              0\n",
            "ma_w            0\n",
            "macd_w          0\n",
            "macdsignal_w    0\n",
            "macdhist_w      0\n",
            "rsi_w           0\n",
            "ad_w            0\n",
            "label           0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wHy2cbYXKbG",
        "outputId": "98ee1efb-e068-4bf8-e9eb-7a671b13b3ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'date', 'open', 'high', 'low', 'close', 'volume',\n",
            "       'trade_volume', 'ma', 'macd', 'macdsignal', 'macdhist', 'rsi', 'ad',\n",
            "       'ma_w', 'macd_w', 'macdsignal_w', 'macdhist_w', 'rsi_w', 'ad_w',\n",
            "       'label'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['ma_w', 'macd_w', 'macdsignal_w', 'macdhist_w', 'rsi_w', 'ad_w']]\n",
        "y = df ['label']"
      ],
      "metadata": {
        "id": "JpU8ra8_WDCN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.value_counts())\n",
        "plt.figure(figsize=(5, 5))\n",
        "axl = sns.countplot(y)\n",
        "axl.set_title(\"target\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "P_KzNjj0VeoS",
        "outputId": "1ea9c5c2-921e-4d6a-a3ad-69896b1725ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H    92569\n",
            "B     2176\n",
            "S     2176\n",
            "Name: label, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'target')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAFNCAYAAACqtRxWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATnElEQVR4nO3df7BcZX3H8ffHRFREfkmKmlBDa0YbHIuQQawz7SiOhFaNWnTwR4mWMXaEtnZsLbad0rHS1tbWij9oqUTAMgKilmijDMUfnbaCJIJioIy3CCYZkEgCqK1g4rd/7BNc4024gTx37937fs3s5Oxzztl99g68Z++5Z8+mqpAk9fGoUU9AksaZkZWkjoysJHVkZCWpIyMrSR0ZWUnqyMhKUkdGVmMpyW1JXjjXnlszj5GVdpFk3qjnoPFhZDV2knwE+FngU0m+l+RtST6W5M4k9yb59yRHDW1/QZJzk6xN8n3g+UmOSXJ9ku+2fS9N8s6hfV6c5IYk9yT5ryTP2t1zT/PL1wxjZDV2quo3gG8BL6mqA6rqr4HPAEuAnwG+Aly8y26vAc4GngB8GfgkcAFwKPBR4OU7N0zybGA18CbgicA/AmuSPGY3z605zMhqTqiq1VX13aq6H/gz4BeTHDS0yRVV9Z9V9SPgaGA+cE5V/bCqPsEgvDutAv6xqq6tqh1VdSFwP3D89LwazSZGVmMvybwkf5Xkf5LcB9zWVh02tNnGoeWnAJvrJ6+eNLz+qcBb26GCe5LcAxzR9pN+gpHVuBoO5GuAFcALgYOAxW08u9n+DmBhkuH1RwwtbwTOrqqDh277V9VHJ3kszXFGVuPq28DPteUnMPh1/m5gf+AvHmLfLwE7gDOSzE+yAjhuaP0/Ab+V5DkZeHySX0vyhEmeW3OckdW4+kvgT9qv8ocCtwObgZuAa/a0Y1U9ALwCOA24B3gd8GkGoaaq1gFvBN4PbAMmgNdP9txJfn/fvSTNRvGi3dJDS3It8A9V9eFRz0Wzi+9kpUkk+ZUkT2qHC1YCzwI+O+p5afaZP+oJSDPU04HLgMcDtwInV9Udo52SZiMPF0hSRx4ukKSOjKwkdTTnjskedthhtXjx4lFPQ9KYWb9+/XeqasGu43MusosXL2bdunWjnoakMZPk9snGPVwgSR0ZWUnqyMhKUkdGVpI6MrKS1JGRlaSOjKwkdWRkJakjIytJHRlZSerIyEpSR3Pu2gWSBj694YOjnsKs8OKj3vyI9vedrCR1ZGQlqSMjK0kdGVlJ6sjISlJHRlaSOjKyktSRkZWkjoysJHVkZCWpIyMrSR0ZWUnqyMhKUkdGVpI6MrKS1JGRlaSOjKwkdWRkJakjIytJHRlZSerIyEpSR0ZWkjoyspLUkZGVpI6MrCR1ZGQlqSMjK0kdGVlJ6sjISlJHRlaSOuoa2SS/l2RDkq8n+WiSxyY5Msm1SSaSXJpkv7btY9r9ibZ+8dDjvL2N35LkxKHx5W1sIsmZPV+LJD0c3SKbZCHwO8CyqnomMA84BXgX8J6qehqwDTit7XIasK2Nv6dtR5Klbb+jgOXAB5PMSzIP+ABwErAUeHXbVpJmjN6HC+YDj0syH9gfuAN4AXB5W38h8LK2vKLdp60/IUna+CVVdX9VfROYAI5rt4mqurWqHgAuadtK0ozRLbJVtRl4N/AtBnG9F1gP3FNV29tmm4CFbXkhsLHtu71t/8Th8V322d24JM0YPQ8XHMLgneWRwFOAxzP4dX/aJVmVZF2SdVu2bBnFFCTNUT0PF7wQ+GZVbamqHwKfAJ4HHNwOHwAsAja35c3AEQBt/UHA3cPju+yzu/GfUlXnVdWyqlq2YMGCffHaJGlKekb2W8DxSfZvx1ZPAG4CPg+c3LZZCVzRlte0+7T1n6uqauOntLMPjgSWAF8GrgOWtLMV9mPwx7E1HV+PJO21+Q+9ycNTVdcmuRz4CrAduB44D/hX4JIk72xj57ddzgc+kmQC2MogmlTVhiSXMQj0duD0qtoBkOQM4EoGZy6srqoNvV6PJD0c3SILUFVnAWftMnwrgzMDdt32B8Ard/M4ZwNnTzK+Flj7yGcqSX34iS9J6sjISlJHRlaSOjKyktSRkZWkjoysJHVkZCWpIyMrSR0ZWUnqyMhKUkdGVpI6MrKS1JGRlaSOjKwkdWRkJakjIytJHRlZSerIyEpSR0ZWkjoyspLUkZGVpI6MrCR1ZGQlqSMjK0kdGVlJ6sjISlJHRlaSOjKyktSRkZWkjoysJHVkZCWpIyMrSR0ZWUnqyMhKUkdGVpI6MrKS1JGRlaSOjKwkdWRkJakjIytJHRlZSerIyEpSR0ZWkjoyspLUkZGVpI6MrCR1ZGQlqaOukU1ycJLLk/x3kpuTPDfJoUmuSvKN9u8hbdskOSfJRJKvJTlm6HFWtu2/kWTl0PixSW5s+5yTJD1fjyTtrd7vZN8LfLaqngH8InAzcCZwdVUtAa5u9wFOApa02yrgXIAkhwJnAc8BjgPO2hnmts0bh/Zb3vn1SNJe6RbZJAcBvwycD1BVD1TVPcAK4MK22YXAy9ryCuCiGrgGODjJk4ETgauqamtVbQOuApa3dQdW1TVVVcBFQ48lSTNCz3eyRwJbgA8nuT7Jh5I8Hji8qu5o29wJHN6WFwIbh/bf1Mb2NL5pknFJmjF6RnY+cAxwblU9G/g+Pz40AEB7B1od5wBAklVJ1iVZt2XLlt5PJ0kP6hnZTcCmqrq23b+cQXS/3X7Vp/17V1u/GThiaP9FbWxP44smGf8pVXVeVS2rqmULFix4RC9KkvZGt8hW1Z3AxiRPb0MnADcBa4CdZwisBK5oy2uAU9tZBscD97bDClcCL0pySPuD14uAK9u6+5Ic384qOHXosSRpRpjf+fF/G7g4yX7ArcAbGIT9siSnAbcDr2rbrgV+FZgA/rdtS1VtTfLnwHVtu3dU1da2/GbgAuBxwGfaTZJmjK6RraobgGWTrDphkm0LOH03j7MaWD3J+DrgmY9wmpLUjZ/4kqSOjKwkdWRkJakjIytJHRlZSerIyEpSR0ZWkjoyspLUkZGVpI6MrCR1ZGQlqSMjK0kdGVlJ6sjISlJHRlaSOppSZJNcPZUxSdJP2uNFu5M8FtgfOKx99UvaqgPxm2El6SE91DcjvAl4C/AUYD0/jux9wPs7zkuSxsIeI1tV7wXem+S3q+p90zQnSRobU/qOr6p6X5JfAhYP71NVF3WalySNhSlFNslHgJ8HbgB2tOECjKwk7cFUv612GbC0faOsJGmKpnqe7NeBJ/WciCSNo6m+kz0MuCnJl4H7dw5W1Uu7zEqSxsRUI/tnPSchSeNqqmcXfLH3RCRpHE317ILvMjibAGA/4NHA96vqwF4Tk6RxMNV3sk/YuZwkwArg+F6TkqRxsddX4aqBfwFO7DAfSRorUz1c8Iqhu49icN7sD7rMSJLGyFTPLnjJ0PJ24DYGhwwkSXsw1WOyb+g9EUkaR1O9aPeiJJ9Mcle7fTzJot6Tk6TZbqp/+PowsIbBdWWfAnyqjUmS9mCqkV1QVR+uqu3tdgGwoOO8JGksTDWydyd5XZJ57fY64O6eE5OkcTDVyP4m8CrgTuAO4GTg9Z3mJEljY6qncL0DWFlV2wCSHAq8m0F8JUm7MdV3ss/aGViAqtoKPLvPlCRpfEw1so9qXwkOPPhOdqrvgiVpzppqKP8W+FKSj7X7rwTO7jMlSRofU/3E10VJ1gEvaEOvqKqb+k1LksbDlH/lb1E1rJK0F/b6UoeSpKkzspLUkZGVpI6MrCR11D2y7VoH1yf5dLt/ZJJrk0wkuTTJfm38Me3+RFu/eOgx3t7Gb0ly4tD48jY2keTM3q9FkvbWdLyT/V3g5qH77wLeU1VPA7YBp7Xx04Btbfw9bTuSLAVOAY4ClgMf3HmhGuADwEnAUuDVbVtJmjG6RrZd2PvXgA+1+2Fwru3lbZMLgZe15RXtPm39CUPfjHtJVd1fVd8EJoDj2m2iqm6tqgeAS/ArcSTNML3fyf498DbgR+3+E4F7qmp7u78JWNiWFwIbAdr6e9v2D47vss/uxiVpxugW2SQvBu6qqvW9nmMv5rIqybok67Zs2TLq6UiaQ3q+k30e8NIktzH4Vf4FwHuBg5Ps/KTZImBzW94MHAHQ1h/E4MLgD47vss/uxn9KVZ1XVcuqatmCBX6hg6Tp0y2yVfX2qlpUVYsZ/OHqc1X1WuDzDC76DbASuKItr2n3aes/V1XVxk9pZx8cCSwBvgxcByxpZyvs155jTa/XI0kPxyguV/iHwCVJ3glcD5zfxs8HPpJkAtjKIJpU1YYklzG4bsJ24PSq2gGQ5AzgSmAesLqqNkzrK5GkhzAtka2qLwBfaMu3MjgzYNdtfsDgEoqT7X82k1xasarWAmv34VQlaZ/yE1+S1JGRlaSOjKwkdWRkJakjIytJHRlZSerIyEpSR0ZWkjoyspLUkZGVpI6MrCR1ZGQlqSMjK0kdGVlJ6sjISlJHRlaSOjKyktSRkZWkjoysJHVkZCWpIyMrSR0ZWUnqyMhKUkdGVpI6MrKS1JGRlaSOjKwkdWRkJakjIytJHRlZSerIyEpSR0ZWkjoyspLUkZGVpI6MrCR1ZGQlqSMjK0kdGVlJ6sjISlJHRlaSOjKyktSRkZWkjoysJHVkZCWpIyMrSR0ZWUnqyMhKUkfdIpvkiCSfT3JTkg1JfreNH5rkqiTfaP8e0saT5JwkE0m+luSYocda2bb/RpKVQ+PHJrmx7XNOkvR6PZL0cPR8J7sdeGtVLQWOB05PshQ4E7i6qpYAV7f7ACcBS9ptFXAuDKIMnAU8BzgOOGtnmNs2bxzab3nH1yNJe61bZKvqjqr6Slv+LnAzsBBYAVzYNrsQeFlbXgFcVAPXAAcneTJwInBVVW2tqm3AVcDytu7Aqrqmqgq4aOixJGlGmJZjskkWA88GrgUOr6o72qo7gcPb8kJg49Bum9rYnsY3TTIuSTNG98gmOQD4OPCWqrpveF17B1rTMIdVSdYlWbdly5beTydJD+oa2SSPZhDYi6vqE2342+1Xfdq/d7XxzcARQ7svamN7Gl80yfhPqarzqmpZVS1bsGDBI3tRkrQXep5dEOB84Oaq+ruhVWuAnWcIrASuGBo/tZ1lcDxwbzuscCXwoiSHtD94vQi4sq27L8nx7blOHXosSZoR5nd87OcBvwHcmOSGNvZHwF8BlyU5DbgdeFVbtxb4VWAC+F/gDQBVtTXJnwPXte3eUVVb2/KbgQuAxwGfaTdJmjG6Rbaq/gPY3XmrJ0yyfQGn7+axVgOrJxlfBzzzEUxTkrryE1+S1JGRlaSOjKwkdWRkJakjIytJHRlZSerIyEpSR0ZWkjoyspLUkZGVpI6MrCR1ZGQlqSMjK0kdGVlJ6sjISlJHRlaSOjKyktSRkZWkjoysJHVkZCWpIyMrSR0ZWUnqyMhKUkdGVpI6MrKS1JGRlaSOjKwkdWRkJakjIytJHRlZSerIyEpSR0ZWkjoyspLUkZGVpI6MrCR1ZGQlqSMjK0kdGVlJ6sjISlJHRlaSOjKyktSRkZWkjoysJHVkZCWpIyMrSR3NH/UEZqJPrp8Y9RRmhZcf+7R99ljH/sFF++yxxtn6vzl11FPQXvKdrCR1NOsjm2R5kluSTCQ5c9TzkaRhszqySeYBHwBOApYCr06ydLSzkqQfm9WRBY4DJqrq1qp6ALgEWDHiOUnSg2Z7ZBcCG4fub2pjkjQjzImzC5KsAla1u99Lcsso5/MwHQZ8Z9STmGNm3M8871456in0NuN+5nD6VDd86mSDsz2ym4Ejhu4vamM/oarOA86brkn1kGRdVS0b9TzmEn/m028cf+az/XDBdcCSJEcm2Q84BVgz4jlJ0oNm9TvZqtqe5AzgSmAesLqqNox4WpL0oFkdWYCqWgusHfU8psGsPtwxS/kzn35j9zNPVY16DpI0tmb7MVlJmtGM7AyXZEeSG5J8NclXkvzSqOc0FyT54yQbknyt/fyfM+o5jask39vl/uuTvH9U89nXZv0x2Tng/6rqaIAkJwJ/CfzKaKc03pI8F3gxcExV3Z/kMGC/EU9Ls5SRnV0OBLaNehJzwJOB71TV/QBVNcNOjtds4h++ZrgkO4Abgccy+J//BVW1frSzGm9JDgD+A9gf+Dfg0qr64mhnNb6G/hvf6VBgTVWdMaIp7VMek535/q+qjq6qZwDLgYuSZNSTGmdV9T3gWAYfxd4CXJrk9SOd1Hjb+d/40e3Q2J+OekL7kpGdRarqSww+271g1HMZd1W1o6q+UFVnAWcAvz7qOWl2MrKzSJJnMPhk292jnss4S/L0JEuGho4Gbh/VfDS7+Yevme9xSW5oywFWVtWOUU5oDjgAeF+Sg4HtwAQ/voqbtFf8w5ckdeThAknqyMhKUkdGVpI6MrKS1JGRlaSOjKzmlF2v+DTJ+sVJvr6Xj3lBkpMf2cw0roysJHVkZDUnJTkgydXtGr03JlkxtHp+kouT3Jzk8iT7t32OTfLFJOuTXJnkySOavmYRI6u56gfAy6vqGOD5wN8OXXjn6cAHq+oXgPuANyd5NPA+4OSqOhZYDZw9gnlrlvFjtZqrAvxFkl8GfgQsBA5v6zZW1X+25X8Gfgf4LPBM4KrW4nnAHdM6Y81KRlZz1WsZXM3s2Kr6YZLbGFyzF2DXz5oXgyhvqKrnTt8UNQ48XKC56iDgrhbY5wNPHVr3s+0raABew+AC3rcAC3aOJ3l0kqOmdcaalYys5qqLgWVJbgROBf57aN0twOlJbgYOAc6tqgeAk4F3JfkqcAPgl1rqIXkVLknqyHeyktSRkZWkjoysJHVkZCWpIyMrSR0ZWUnqyMhKUkdGVpI6+n8S8f5RpbG58gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Holding:', round(y.value_counts()['H']/len(y),4))\n",
        "print('Buy:', round(y.value_counts()['B']/len(y),4))\n",
        "print('Sell:', round(y.value_counts()['S']/len(y),4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quQYPGerZuki",
        "outputId": "dc2701c7-adae-4928-f449-4e5b281d923a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Holding: 0.9551\n",
            "Buy: 0.0225\n",
            "Sell: 0.0225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN5HVA0YK5J4",
        "outputId": "8bfd1fb7-cbaa-46c6-9847-8bf79b1cb58b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.callbacks import Callback,ModelCheckpoint,ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "j490GpgSdQuw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 기간 10월 1일 ~ 12월 9일\n",
        "onehot_encoder = OneHotEncoder()\n",
        "y_onehot = onehot_encoder.fit_transform(y.values.reshape(-1, 1)).toarray()\n",
        "X_test = X[78586:]\n",
        "y_test = y_onehot[78586:]\n",
        "X_data = X[:78586]\n",
        "y_data = y_onehot[:78586]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, shuffle=True,\n",
        "                                                      stratify=y_data, random_state=12)"
      ],
      "metadata": {
        "id": "ncWXbCklZ3jQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "xGTe5ALsWdk5",
        "outputId": "5a326af6-ba7c-4103-b05b-ba647f2bff34"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            ma_w     macd_w  macdsignal_w  macdhist_w      rsi_w          ad_w\n",
              "24597  12.200000   6.698147      6.332111    0.366036   1.320197   4081.215789\n",
              "22630   2.293333 -10.087122     -3.397035   -6.690088 -11.309699  -2331.369761\n",
              "14738 -13.830000  -2.690497     -2.569892   -0.120605   3.882245  -8472.948522\n",
              "50498 -33.816667  -9.113199     -6.859064   -2.254136  -2.187252  49286.320000\n",
              "21166   2.953333  -8.038478     -2.893477   -5.145001  -7.889032 -14234.133333\n",
              "...          ...        ...           ...         ...        ...           ...\n",
              "69815  -9.383333   2.328874      1.505066    0.823808   3.297338   3015.000000\n",
              "46214  -3.566667   4.536408      3.234773    1.301635  -3.286072  -8043.066667\n",
              "51354  12.936667  -1.640134      7.595263   -9.235396 -16.883311 -12450.397143\n",
              "70608   2.800000  -2.081730     -2.582127    0.500397   3.076279   3603.833333\n",
              "15024  -4.760000  12.823529      8.026058    4.797471  17.094208  20478.944444\n",
              "\n",
              "[62868 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae1474d8-525d-4e9f-9b04-8d2b68bb8d2a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ma_w</th>\n",
              "      <th>macd_w</th>\n",
              "      <th>macdsignal_w</th>\n",
              "      <th>macdhist_w</th>\n",
              "      <th>rsi_w</th>\n",
              "      <th>ad_w</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24597</th>\n",
              "      <td>12.200000</td>\n",
              "      <td>6.698147</td>\n",
              "      <td>6.332111</td>\n",
              "      <td>0.366036</td>\n",
              "      <td>1.320197</td>\n",
              "      <td>4081.215789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22630</th>\n",
              "      <td>2.293333</td>\n",
              "      <td>-10.087122</td>\n",
              "      <td>-3.397035</td>\n",
              "      <td>-6.690088</td>\n",
              "      <td>-11.309699</td>\n",
              "      <td>-2331.369761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14738</th>\n",
              "      <td>-13.830000</td>\n",
              "      <td>-2.690497</td>\n",
              "      <td>-2.569892</td>\n",
              "      <td>-0.120605</td>\n",
              "      <td>3.882245</td>\n",
              "      <td>-8472.948522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50498</th>\n",
              "      <td>-33.816667</td>\n",
              "      <td>-9.113199</td>\n",
              "      <td>-6.859064</td>\n",
              "      <td>-2.254136</td>\n",
              "      <td>-2.187252</td>\n",
              "      <td>49286.320000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21166</th>\n",
              "      <td>2.953333</td>\n",
              "      <td>-8.038478</td>\n",
              "      <td>-2.893477</td>\n",
              "      <td>-5.145001</td>\n",
              "      <td>-7.889032</td>\n",
              "      <td>-14234.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69815</th>\n",
              "      <td>-9.383333</td>\n",
              "      <td>2.328874</td>\n",
              "      <td>1.505066</td>\n",
              "      <td>0.823808</td>\n",
              "      <td>3.297338</td>\n",
              "      <td>3015.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46214</th>\n",
              "      <td>-3.566667</td>\n",
              "      <td>4.536408</td>\n",
              "      <td>3.234773</td>\n",
              "      <td>1.301635</td>\n",
              "      <td>-3.286072</td>\n",
              "      <td>-8043.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51354</th>\n",
              "      <td>12.936667</td>\n",
              "      <td>-1.640134</td>\n",
              "      <td>7.595263</td>\n",
              "      <td>-9.235396</td>\n",
              "      <td>-16.883311</td>\n",
              "      <td>-12450.397143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70608</th>\n",
              "      <td>2.800000</td>\n",
              "      <td>-2.081730</td>\n",
              "      <td>-2.582127</td>\n",
              "      <td>0.500397</td>\n",
              "      <td>3.076279</td>\n",
              "      <td>3603.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15024</th>\n",
              "      <td>-4.760000</td>\n",
              "      <td>12.823529</td>\n",
              "      <td>8.026058</td>\n",
              "      <td>4.797471</td>\n",
              "      <td>17.094208</td>\n",
              "      <td>20478.944444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62868 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae1474d8-525d-4e9f-9b04-8d2b68bb8d2a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae1474d8-525d-4e9f-9b04-8d2b68bb8d2a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae1474d8-525d-4e9f-9b04-8d2b68bb8d2a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYZYut_kWjG9",
        "outputId": "42a2b23b-94aa-445a-d670-d7ca4c1cd70b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(layers.Dropout(rate=0.2))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))"
      ],
      "metadata": {
        "id": "mwCUDPb_JQNR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "model.compile(optimizer=optimizers.Adagrad(lr=0.01),\n",
        "              loss=tfa.losses.SigmoidFocalCrossEntropy(),\n",
        "              metrics=[tfa.metrics.F1Score(3)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c45FjnTI8s-",
        "outputId": "25c45c01-baf0-4a75-a85b-e4ac94ccf1e5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adagrad.py:77: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adagrad, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7yKW8yPK-ut",
        "outputId": "358b6e59-9de5-41f5-f4ed-9be8cc37fb77"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 64)                448       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,627\n",
            "Trainable params: 2,627\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[reduce_lr],\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SugueSXcI7cy",
        "outputId": "e2cacc52-50f9-4f6b-fb8e-2b7ecbf1408f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1965/1965 [==============================] - 6s 3ms/step - loss: 1.3622 - f1_score: 0.3304 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6507 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0020\n",
            "Epoch 8/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6505 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0020\n",
            "Epoch 9/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6506 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0020\n",
            "Epoch 10/100\n",
            "1965/1965 [==============================] - 6s 3ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0020\n",
            "Epoch 11/100\n",
            "1965/1965 [==============================] - 15s 8ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0020\n",
            "Epoch 12/100\n",
            "1965/1965 [==============================] - 9s 5ms/step - loss: 0.6506 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6497 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6507 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6508 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "1965/1965 [==============================] - 7s 3ms/step - loss: 0.6505 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "1965/1965 [==============================] - 12s 6ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "1965/1965 [==============================] - 8s 4ms/step - loss: 0.6505 - f1_score: 0.3266 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "1965/1965 [==============================] - 4s 2ms/step - loss: 0.6506 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "1965/1965 [==============================] - 4s 2ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "1965/1965 [==============================] - 4s 2ms/step - loss: 0.6505 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6514 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6499 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6508 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6508 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "1965/1965 [==============================] - 4s 2ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6506 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6499 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 41/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 42/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 43/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 44/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 45/100\n",
            "1965/1965 [==============================] - 10s 5ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 46/100\n",
            "1965/1965 [==============================] - 7s 4ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 47/100\n",
            "1965/1965 [==============================] - 9s 4ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 48/100\n",
            "1965/1965 [==============================] - 7s 4ms/step - loss: 0.6505 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 49/100\n",
            "1965/1965 [==============================] - 6s 3ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 50/100\n",
            "1965/1965 [==============================] - 7s 4ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 51/100\n",
            "1965/1965 [==============================] - 7s 3ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 52/100\n",
            "1965/1965 [==============================] - 9s 4ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 53/100\n",
            "1965/1965 [==============================] - 9s 5ms/step - loss: 0.6508 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 54/100\n",
            "1965/1965 [==============================] - 7s 4ms/step - loss: 0.6508 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 55/100\n",
            "1965/1965 [==============================] - 6s 3ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 56/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 57/100\n",
            "1965/1965 [==============================] - 8s 4ms/step - loss: 0.6507 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 58/100\n",
            "1965/1965 [==============================] - 11s 5ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 59/100\n",
            "1965/1965 [==============================] - 8s 4ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 60/100\n",
            "1965/1965 [==============================] - 7s 3ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 61/100\n",
            "1965/1965 [==============================] - 7s 3ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 62/100\n",
            "1965/1965 [==============================] - 9s 5ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 63/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 64/100\n",
            "1965/1965 [==============================] - 4s 2ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 65/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 66/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 67/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 68/100\n",
            "1965/1965 [==============================] - 4s 2ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 69/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 70/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 71/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 72/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6507 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 73/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 74/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 75/100\n",
            "1965/1965 [==============================] - 6s 3ms/step - loss: 0.6500 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 76/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6507 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 77/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6505 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 78/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 79/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6507 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 80/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6510 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 81/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6508 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 82/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6500 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 83/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 84/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6506 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 85/100\n",
            "1965/1965 [==============================] - 6s 3ms/step - loss: 0.6505 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 86/100\n",
            "1965/1965 [==============================] - 6s 3ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 87/100\n",
            "1965/1965 [==============================] - 7s 3ms/step - loss: 0.6500 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 88/100\n",
            "1965/1965 [==============================] - 6s 3ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 89/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 90/100\n",
            "1965/1965 [==============================] - 6s 3ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 91/100\n",
            "1965/1965 [==============================] - 6s 3ms/step - loss: 0.6505 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 92/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 93/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 94/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6503 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 95/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6504 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 96/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 97/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6501 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 98/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6505 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 99/100\n",
            "1965/1965 [==============================] - 5s 2ms/step - loss: 0.6502 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n",
            "Epoch 100/100\n",
            "1965/1965 [==============================] - 5s 3ms/step - loss: 0.6505 - f1_score: 0.3261 - val_loss: 0.6466 - val_f1_score: 0.3261 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.ylim([0.6, 0.7])\n",
        "plt.plot(epochs, loss, '.', label='Training loss', color='lightgreen')\n",
        "plt.plot(epochs, val_loss, '-', label='Validation loss', color='lightblue')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "5OozfnwRO3c0",
        "outputId": "2cdd0f2a-5688-46fb-fcc3-089b20ccdae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wU9Z3u8c/DDDgKKCpokCERN+Atym3AC5GguWF0RYlRWY9C3Gg0FxONGpJsImvieZ2z8ey6nGB20URjVkM8JsvBqMEbiteEQYnKRUXEdRQVR64BhIHv/lE1Y9NUz/RcegZmnrcvXnZX/6r6W1U9/XT9qvtXigjMzMzydevoAszMbPfkgDAzs0wOCDMzy+SAMDOzTA4IMzPL5IAwM7NMDghrF5LulzS5rdt2JEkrJX2mBMsNSR9Pb/+bpB8W07YFz3O+pAdaWmcjyx0nqaatl2vtr7yjC7Ddl6SNOXf3AT4Atqf3vxoRdxS7rIg4tRRtO7uIuLQtliPpUOA1oHtE1KXLvgMoeh9a1+OAsIIiolf9bUkrga9ExEP57SSV17/pmFnn4S4ma7b6LgRJ35X0NnCrpP0l/UHSaklr0tuVOfM8Kukr6e0pkp6QdEPa9jVJp7aw7SBJ8yVtkPSQpBmS/qNA3cXU+GNJT6bLe0BS35zHL5D0uqRaST9oZPscJ+ltSWU5086S9Hx6e7SkpyWtlbRK0s8k9SiwrNsk/STn/tXpPG9Juiiv7WmSnpO0XtIbkqblPDw//f9aSRslnVC/bXPmP1HSAknr0v+fWOy2aYykI9P510paLOmMnMe+IGlJusw3JV2VTu+b7p+1kt6X9Lgkv1+1M29wa6mPAAcAHwMuIXkt3Zre/yiwGfhZI/MfB7wE9AX+CfiFJLWg7Z3An4EDgWnABY08ZzE1/h3wZeAgoAdQ/4Z1FPDzdPmHpM9XSYaI+BPwV+CUvOXemd7eDlyRrs8JwKeBrzVSN2kN49N6PgsMBvLPf/wVuBDoA5wGXCbpzPSxsen/+0REr4h4Om/ZBwD3AtPTdftn4F5JB+atwy7bpomauwP3AA+k830TuEPS4WmTX5B0V/YGPgE8kk7/DlAD9AMOBr4PeFygduaAsJbaAVwbER9ExOaIqI2I30XEpojYAFwPfKqR+V+PiJsjYjvwK6A/yRtB0W0lfRQYBfwoIrZGxBPAnEJPWGSNt0bEyxGxGbgLGJZOPxv4Q0TMj4gPgB+m26CQ3wCTACT1Br6QTiMiFkbEMxFRFxErgX/PqCPLOWl9L0bEX0kCMXf9Ho2IFyJiR0Q8nz5fMcuFJFBeiYhfp3X9BlgG/G1Om0LbpjHHA72A/5Xuo0eAP5BuG2AbcJSkfSNiTUQ8mzO9P/CxiNgWEY+HB45rdw4Ia6nVEbGl/o6kfST9e9oFs56kS6NPbjdLnrfrb0TEpvRmr2a2PQR4P2cawBuFCi6yxrdzbm/KqemQ3GWnb9C1hZ6L5GhhoqS9gInAsxHxelrHkLT75O20jv9JcjTRlJ1qAF7PW7/jJM1Lu9DWAZcWudz6Zb+eN+11YEDO/ULbpsmaIyI3THOX+0WS8Hxd0mOSTkin/xRYDjwgaYWkqcWthrUlB4S1VP6nue8AhwPHRcS+fNilUajbqC2sAg6QtE/OtIGNtG9Njatyl50+54GFGkfEEpI3wlPZuXsJkq6qZcDgtI7vt6QGkm6yXHeSHEENjIj9gH/LWW5Tn77fIul6y/VR4M0i6mpquQPzzh80LDciFkTEBJLup9kkRyZExIaI+E5EHAacAVwp6dOtrMWayQFhbaU3SZ/+2rQ/+9pSP2H6ibwamCapR/rp828bmaU1Nd4NnC7pk+kJ5eto+u/nTuBbJEH0//LqWA9slHQEcFmRNdwFTJF0VBpQ+fX3Jjmi2iJpNEkw1VtN0iV2WIFl3wcMkfR3ksolnQscRdId1Bp/IjnauEZSd0njSPbRrHSfnS9pv4jYRrJNdgBIOl3Sx9NzTetIzts01qVnJeCAsLZyI7A38B7wDPDHdnre80lO9NYCPwF+S/J7jSwtrjEiFgNfJ3nTXwWsITmJ2pj6cwCPRMR7OdOvInnz3gDcnNZcTA33p+vwCEn3yyN5Tb4GXCdpA/Aj0k/j6bybSM65PJl+M+j4vGXXAqeTHGXVAtcAp+fV3WwRsZUkEE4l2e43ARdGxLK0yQXAyrSr7VKS/QnJSfiHgI3A08BNETGvNbVY88nnfawzkfRbYFlElPwIxqyz8xGE7dEkjZL0N5K6pV8DnUDSl21mrVTSgJA0XtJLkpZnfQtB0r9IWpT+e1nS2pzHJkt6Jf2324/LYx3mI8CjJF0R04HLIuK5Dq3IrJMoWRdT+tXBl0l+1FMDLAAmpd/uyGr/TWB4RFyUnkCsBqpIvn2xEBgZEWtKUqyZme2ilEcQo4HlEbEiPVE1i+Twv5BJpD8kAj4PPBgR76eh8CAwvoS1mplZnlIO1jeAnX/UU0MyZMIuJH0MGMSH38rImndAxnyXkAzzQM+ePUceccQRra/azKwLWbhw4XsR0S/rsd1lNNfzgLvToRSKFhEzgZkAVVVVUV1dXYrazMw6LUn5v6BvUMoupjfZ+VeflRT+VeZ5fNi91Nx5zcysBEoZEAuAwUqGY+5BEgK7DKSW/pJ0f5Ifw9SbC3xOyfDM+wOfS6eZmVk7KVkXU0TUSfoGyRt7GfDLiFgs6TqgOiLqw+I8YFbuSI0R8b6kH5OEDMB1EfF+qWo1M7NddZpfUvschFn727ZtGzU1NWzZsqXpxtahKioqqKyspHv37jtNl7QwIqqy5tldTlKb2R6opqaG3r17c+ihh1L4ek/W0SKC2tpaampqGDRoUNHzeagNM2uxLVu2cOCBBzocdnOSOPDAA5t9pOeAMLNWcTjsGVqynxwQZmaWyQFhZnus2tpahg0bxrBhw/jIRz7CgAEDGu5v3bq10Xmrq6u5/PLLm3yOE088sU1qffTRRzn99NPbZFntxSepzWyPdeCBB7Jo0SIApk2bRq9evbjqqqsaHq+rq6O8PPttrqqqiqqqzC/v7OSpp55qm2L3QD6CMLN2tapuFQs2L2BV3aqSLH/KlClceumlHHfccVxzzTX8+c9/5oQTTmD48OGceOKJvPTSS8DOn+inTZvGRRddxLhx4zjssMOYPn16w/J69erV0H7cuHGcffbZHHHEEZx//vnU/0zgvvvu44gjjmDkyJFcfvnlTR4pvP/++5x55pkce+yxHH/88Tz//PMAPPbYYw1HQMOHD2fDhg2sWrWKsWPHMmzYMD7xiU/w+OOPt/k2K8RHEGbWblbVreL3G37PdrZTtqWMib0n0r+8f5s/T01NDU899RRlZWWsX7+exx9/nPLych566CG+//3v87vf/W6XeZYtW8a8efPYsGEDhx9+OJdddtkuvxl47rnnWLx4MYcccghjxozhySefpKqqiq9+9avMnz+fQYMGMWnSpCbru/baaxk+fDizZ8/mkUce4cILL2TRokXccMMNzJgxgzFjxrBx40YqKiqYOXMmn//85/nBD37A9u3b2bRpU5ttp6Y4IMys3dRsq2E72wmC7WynZltNSQLiS1/6EmVlZQCsW7eOyZMn88orryCJbdu2Zc5z2mmnsddee7HXXntx0EEH8c4771BZWblTm9GjRzdMGzZsGCtXrqRXr14cdthhDb8vmDRpEjNnzmy0vieeeKIhpE455RRqa2tZv349Y8aM4corr+T8889n4sSJVFZWMmrUKC666CK2bdvGmWeeybBhw1q1bZrDXUxm1m4qu1dSRhlClFFGZffKpmdqgZ49ezbc/uEPf8jJJ5/Miy++yD333FPwtwB77bVXw+2ysjLq6upa1KY1pk6dyi233MLmzZsZM2YMy5YtY+zYscyfP58BAwYwZcoUbr/99jZ9zsY4IMys3fQv78/E3hM5oeKEknUv5Vu3bh0DBiSXk7ntttvafPmHH344K1asYOXKlQD89re/bXKek046iTvuuANIzm307duXfffdl1dffZVjjjmG7373u4waNYply5bx+uuvc/DBB3PxxRfzla98hWeffbbN16EQdzGZWbvqX96/XYKh3jXXXMPkyZP5yU9+wmmnndbmy99777256aabGD9+PD179mTUqFFNzlN/UvzYY49ln3324Ve/+hUAN954I/PmzaNbt24cffTRnHrqqcyaNYuf/vSndO/enV69erXrEYQH6zOzFlu6dClHHnlkR5fR4TZu3EivXr2ICL7+9a8zePBgrrjiio4uaxdZ+6uxwfrcxWRm1ko333wzw4YN4+ijj2bdunV89atf7eiS2oS7mMzMWumKK67YLY8YWstHEGZmlskBYWZmmRwQZmaWyQFhZmaZHBBmtsc6+eSTmTt37k7TbrzxRi677LKC84wbN476r8R/4QtfYO3atbu0mTZtGjfccEOjzz179myWLFnScP9HP/oRDz30UHPKz7Q7DQvugDCzPdakSZOYNWvWTtNmzZpV1IB5kIzC2qdPnxY9d35AXHfddXzmM59p0bJ2Vw4IM9tjnX322dx7770NFwdauXIlb731FieddBKXXXYZVVVVHH300Vx77bWZ8x966KG89957AFx//fUMGTKET37ykw1DgkPyG4dRo0YxdOhQvvjFL7Jp0yaeeuop5syZw9VXX82wYcN49dVXmTJlCnfffTcADz/8MMOHD+eYY47hoosu4oMPPmh4vmuvvZYRI0ZwzDHHsGzZskbXr6OHBffvIMysTfzl3XWs29K2g9ftV1HO0IP2K/j4AQccwOjRo7n//vuZMGECs2bN4pxzzkES119/PQcccADbt2/n05/+NM8//zzHHnts5nIWLlzIrFmzWLRoEXV1dYwYMYKRI0cCMHHiRC6++GIA/uEf/oFf/OIXfPOb3+SMM87g9NNP5+yzz95pWVu2bGHKlCk8/PDDDBkyhAsvvJCf//znfPvb3wagb9++PPvss9x0003ccMMN3HLLLQXXr6OHBfcRhJnt0XK7mXK7l+666y5GjBjB8OHDWbx48U7dQfkef/xxzjrrLPbZZx/23XdfzjjjjIbHXnzxRU466SSOOeYY7rjjDhYvXtxoPS+99BKDBg1iyJAhAEyePJn58+c3PD5x4kQARo4c2TDAXyFPPPEEF1xwAZA9LPj06dNZu3Yt5eXljBo1iltvvZVp06bxwgsv0Lt370aXXQwfQZhZm2jsk34pTZgwgSuuuIJnn32WTZs2MXLkSF577TVuuOEGFixYwP7778+UKVMKDvPdlClTpjB79myGDh3KbbfdxqOPPtqqeuuHDG/NcOFTp07ltNNO47777mPMmDHMnTu3YVjwe++9lylTpnDllVdy4YUXtqpWH0GY2R6tV69enHzyyVx00UUNRw/r16+nZ8+e7Lfffrzzzjvcf//9jS5j7NixzJ49m82bN7Nhwwbuueeehsc2bNhA//792bZtW8MQ3QC9e/dmw4YNuyzr8MMPZ+XKlSxfvhyAX//613zqU59q0bp19LDgPoIwsz3epEmTOOussxq6moYOHcrw4cM54ogjGDhwIGPGjGl0/hEjRnDuuecydOhQDjrooJ2G7P7xj3/McccdR79+/TjuuOMaQuG8887j4osvZvr06Q0npwEqKiq49dZb+dKXvkRdXR2jRo3i0ksvbdF6dfSw4B7u28xazMN971k83LeZmbUJB4SZmWVyQJhZq3SWburOriX7yQFhZi1WUVFBbW2tQ2I3FxHU1tZSUVHRrPn8LSYza7HKykpqampYvXp1R5diTaioqKCysrJZ8zggzKzFunfvzqBBgzq6DCsRdzGZmVmmkgaEpPGSXpK0XNLUAm3OkbRE0mJJd+ZM/6d02lJJ0yWplLWamdnOStbFJKkMmAF8FqgBFkiaExFLctoMBr4HjImINZIOSqefCIwB6odefAL4FPBoqeo1M7OdlfIIYjSwPCJWRMRWYBYwIa/NxcCMiFgDEBHvptMDqAB6AHsB3YF3SlirmZnlKWVADADeyLlfk07LNQQYIulJSc9IGg8QEU8D84BV6b+5EbE0/wkkXSKpWlK1v0VhZta2OvokdTkwGBgHTAJultRH0seBI4FKklA5RdJJ+TNHxMyIqIqIqn79+rVj2WZmnV8pA+JNYGDO/cp0Wq4aYE5EbIuI14CXSQLjLOCZiNgYERuB+4ETSlirmZnlKWVALAAGSxokqQdwHjAnr81skqMHJPUl6XJaAfwX8ClJ5ZK6k5yg3qWLyczMSqdkARERdcA3gLkkb+53RcRiSddJqr+e31ygVtISknMOV0dELXA38CrwAvAX4C8Rcc8uT2JmZiXj60GYmXVhvh6EmZk1mwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDKVNCAkjZf0kqTlkqYWaHOOpCWSFku6M2f6RyU9IGlp+vihpazVzMx2Vl6qBUsqA2YAnwVqgAWS5kTEkpw2g4HvAWMiYo2kg3IWcTtwfUQ8KKkXsKNUtZqZ2a5KeQQxGlgeESsiYiswC5iQ1+ZiYEZErAGIiHcBJB0FlEfEg+n0jRGxqYS1mplZnlIGxADgjZz7Nem0XEOAIZKelPSMpPE509dK+r2k5yT9ND0i2YmkSyRVS6pevXp1SVbCzKyr6uiT1OXAYGAcMAm4WVKfdPpJwFXAKOAwYEr+zBExMyKqIqKqX79+7VWzmVmXUMqAeBMYmHO/Mp2WqwaYExHbIuI14GWSwKgBFqXdU3XAbGBECWs1M7M8pQyIBcBgSYMk9QDOA+bktZlNcvSApL4kXUsr0nn7SKo/LDgFWIKZmbWbkgVE+sn/G8BcYClwV0QslnSdpDPSZnOBWklLgHnA1RFRGxHbSbqXHpb0AiDg5lLVamZmu1JEdHQNbaKqqiqqq6s7ugwzsz2KpIURUZX1WEefpDYzs92UA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDIVFRCSekrqlt4eIukMSd1LW5qZmXWkYo8g5gMVkgYADwAXALeVqigzM+t4xQaE0gv2TARuiogvAUeXriwzM+toRQeEpBOA84F702m7XMDHrJBVdatYsHkBq+pWdXQpZlakYq9J/W2Sa0f/Zzoi62Eko6/aHmpV3SpqttVQ2b2S/uX9S/5cv9/we7aznbItZUzsPTHzOXNrAtqtvo7WVde7KS15jXpbtq2iAiIiHgMeA0hPVr8XEZeXsrCOUugFVuh2oRdeMcvJnTf/j6HQH0dr6quft6JbBfM3zc98wy7meZtb34YdG9jOdoJgO9up2Vazy3bLDZFuW5ID2x3saHF9hTT3DaSY9s3dJ/m3m7vexdTR3G3T2jfj1my/QvPlfqgYu89YtuzY0uS2b822LHS7NduysXVuzWuxJa/9ligqICTdCVwKbCe5mM++kv41In7aptV0sEJvUoVu579wgV3egIuZN/8Ne+w+Y3e5n9+u2Prq/zBy102IHewAYDvbWfrB0szgaLP60v92sIMyyqjoVsGCzQsaDZF6La0v980kKxhbsx9bs0/yl3NkjyObtd7NeQ01tW2y1rOx7VfsPM3ZfoXemHNfD3XU8eimRwmi0XVrzbZs6d9pc9Z5Yu+JLd42xe7fQkfnLVXU9SAkLYqIYZLOJ7n051RgYUQc22aVtFJbXA9iweYFPL3laYLir5HRjW4EQTc+3Mm5b8DFzJvbXoiB5QN5o+6Nhjqy2hVDiE/0+AS9u/Vmw44NvLj1xV2W2VjdbVVfbh07/WGwc4g0dbuY+nKnl7FzmDW2/Zq7H1u6T7K2y9KtSxvdHi2po6npxWzX/O1XzDzNrbut9ntbbMtCWrMt6+XX19rXYqFtcELFCYzae1Sz1q+x60EUew6ie/q7hzOBn0XENkmd40pDOSq7V1K2pazoN6/8T+L16nduY38AufPmti+jjI/3+Dhv1b21ywupmOXm316ydUnD7dxP8fWfQHKDI3f5bVlfGWUcudeR9C/vz4LNCzI/5e1gR0OI5H+SbE59+ftk+dblDc9XqL7m7sfW7pP87XLkXkdmfoLOWu9i6ih222StZ2Pbr9h5mrP98j/pZ70ecj9UNPa6bM22bO7faXPXuSz9Tk9bvBYb2wb1691Wig2IfwdWAn8B5kv6GLC+TSvZDfQv78/E3hOL7p8s9Gk49w24mHnz2/cv70/fsr5Ntmuqvtw/jPw34Nz+2PpPNbnLb+v66p+vUAjnhkju/mhuffnTc8OsUH3N3Y+t2SeFtkux611MHcVML7SejW2/YuZp7vZrKjzrt0sxr7fWbMvm/p229G+/Ofu02H1U6PxMW2jxJUcllafXnd4tdNQlR1vzrYmWnFhtzgtgpxN9FPftodae+G3uekPbn5wr9oR6KWpqS605udzcLzkUu/2KmadQm0J157Zv7rxteUK9ufO25G+/rV6Lbfn6a6yLqdhzEPsB1wJj00mPAddFxLpWVdaGfE3qbO35dVYz2/O0xTmIXwIvAuek9y8AbiX5ZbXtxvqX93cwmFmLFBsQfxMRX8y5/4+SFpWiIDMz2z0UGxCbJX0yIp4AkDQG2Fy6strP9h3B23/9oKPLMDNrsR5lot8+e7X5cosNiEuB29NzEQBrgMltXk0HqNuxgz+9taajyzAza7H9K7pz8sc6KCAi4i/AUEn7pvfXS/o28HybV9TOupd145SP9e3oMszMWqy8m0qz3OY0jojc3z5cCdzYtuW0v24SfSp87SMzs3ytueRoaSLLzMx2C60JiE431IaZmX2o0S4mSRvIDgIBe5ekIjMz2y00GhAR0bu9CjEzs91La7qYzMysE3NAmJlZJgeEmZllckCYmVkmB4SZmWUqaUBIGi/pJUnLJU0t0OYcSUskLZZ0Z95j+0qqkfSzUtZpZma7atZQG80hqQyYAXwWqAEWSJoTEUty2gwGvgeMiYg1kg7KW8yPgfmlqtHMzAor5RHEaGB5RKyIiK3ALGBCXpuLgRkRsQYgIt6tf0DSSOBg4IES1mhmZgWUMiAGAG/k3K9Jp+UaAgyR9KSkZySNB5DUDfg/wFWNPYGkSyRVS6pevXp1G5ZuZmYdfZK6HBgMjAMmATdL6gN8DbgvImoamzkiZkZEVURU9evXr+TFmpl1JSU7BwG8CQzMuV+ZTstVA/wpIrYBr0l6mSQwTgBOkvQ1oBfQQ9LGiMg80W1mZm2vlEcQC4DBkgZJ6gGcB8zJazOb5OgBSX1JupxWRMT5EfHRiDiUpJvpdoeDmVn7KllAREQd8A1gLrAUuCsiFku6TtIZabO5QK2kJcA84OqIqC1VTWZmVjxFdI7LOlRVVUV1dXVHl2FmtkeRtDAiqrIe6+iT1GZmtptyQJiZWSYHhJmZZXJAmJlZJgeEmZllckCYmVkmB4SZmWVyQJiZWSYHhJmZZXJAmJlZJgeEmZllckCYmVkmB4SZmWVyQJiZWSYHhJmZZXJAmJlZJgeEmZllckCYmVkmB4SZmWVyQJiZWSYHhJmZZXJAmJlZJgeEmZllckCYmVkmB4SZmWVyQJiZWSYHhJmZZXJAmJlZJgeEmZllckCYmVkmB4SZmWVyQJiZWSYHhJmZZXJAmJlZppIGhKTxkl6StFzS1AJtzpG0RNJiSXem04ZJejqd9rykc0tZp5mZ7aq8VAuWVAbMAD4L1AALJM2JiCU5bQYD3wPGRMQaSQelD20CLoyIVyQdAiyUNDci1paqXjMz21kpjyBGA8sjYkVEbAVmARPy2lwMzIiINQAR8W76/5cj4pX09lvAu0C/EtZqZmZ5ShkQA4A3cu7XpNNyDQGGSHpS0jOSxucvRNJooAfwasZjl0iqllS9evXqNizdzMw6+iR1OTAYGAdMAm6W1Kf+QUn9gV8DX46IHfkzR8TMiKiKiKp+/XyAYWbWlkoZEG8CA3PuV6bTctUAcyJiW0S8BrxMEhhI2he4F/hBRDxTwjrNzCxDKQNiATBY0iBJPYDzgDl5bWaTHD0gqS9Jl9OKtP1/ArdHxN0lrNHMzAooWUBERB3wDWAusBS4KyIWS7pO0hlps7lAraQlwDzg6oioBc4BxgJTJC1K/w0rVa1mZrYrRURH19Amqqqqorq6uqPLMDPbo0haGBFVWY919ElqMzPbTTkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDKVNCAkjZf0kqTlkqYWaHOOpCWSFku6M2f6ZEmvpP8ml7JOMzPbVXmpFiypDJgBfBaoARZImhMRS3LaDAa+B4yJiDWSDkqnHwBcC1QBASxM511TqnrNzGxnpTyCGA0sj4gVEbEVmAVMyGtzMTCj/o0/It5Np38eeDAi3k8fexAYX8JazcwsT8mOIIABwBs592uA4/LaDAGQ9CRQBkyLiD8WmHdA/hNIugS4JL27UdJLzayxL/BeM+fZ03XFdYauud5dcZ2ha653a9b5Y4UeKGVAFKMcGAyMAyqB+ZKOKXbmiJgJzGzpk0uqjoiqls6/J+qK6wxdc7274jpD11zvUq1zKbuY3gQG5tyvTKflqgHmRMS2iHgNeJkkMIqZ18zMSqiUAbEAGCxpkKQewHnAnLw2s0mOHpDUl6TLaQUwF/icpP0l7Q98Lp1mZmbtpGRdTBFRJ+kbJG/sZcAvI2KxpOuA6oiYw4dBsATYDlwdEbUAkn5MEjIA10XE+yUos8XdUwnIPS8AAAVCSURBVHuwrrjO0DXXuyuuM3TN9S7JOisiSrFcMzPbw/mX1GZmlskBYWZmmbpkQBQzBEhnIGmgpHk5Q5l8K51+gKQH02FMHky/CNCpSCqT9JykP6T3B0n6U7rPf5t+caLTkNRH0t2SlklaKumELrKfr0hf2y9K+o2kis64ryX9UtK7kl7MmZa5f5WYnq7/85JGtPR5u1xA5AwBcipwFDBJ0lEdW1XJ1AHfiYijgOOBr6frOhV4OCIGAw+n9zubbwFLc+7/b+BfIuLjwBrg7zukqtL5V+CPEXEEMJRk3Tv1fpY0ALgcqIqIT5B8GeY8Oue+vo1dR5MotH9PJfm5wGCSHxL/vKVP2uUCguKGAOkUImJVRDyb3t5A8qYxgGR9f5U2+xVwZsdUWBqSKoHTgFvS+wJOAe5Om3SqdZa0HzAW+AVARGyNiLV08v2cKgf2llQO7AOsohPu64iYD+R/k7PQ/p0A3B6JZ4A+kvq35Hm7YkAUNYxHZyPpUGA48Cfg4IhYlT70NnBwB5VVKjcC1wA70vsHAmsjoi6939n2+SBgNXBr2q12i6SedPL9HBFvAjcA/0USDOuAhXTufZ2r0P5ts/e4rhgQXY6kXsDvgG9HxPrcxyL5nnOn+a6zpNOBdyNiYUfX0o7KgRHAzyNiOPBX8rqTOtt+Bkj73CeQBOQhQE+66KCepdq/XTEgutQwHpK6k4TDHRHx+3TyO/WHnOn/3y00/x5oDHCGpJUk3YenkPTP90m7IaDz7fMaoCYi/pTev5skMDrzfgb4DPBaRKyOiG3A70n2f2fe17kK7d82e4/rigFRzBAgnULa9/4LYGlE/HPOQ3OA+oswTQb+f3vXVioR8b2IqIyIQ0n27SMRcT4wDzg7bdbZ1vlt4A1Jh6eTPg0soRPv59R/AcdL2id9rdevd6fd13kK7d85wIXpt5mOB9bldEU1S5f8JbWkL5D0U9cPAXJ9B5dUEpI+CTwOvMCH/fHfJzkPcRfwUeB14JwSDWXSoSSNA66KiNMlHUZyRHEA8BzwPyLig46sry1JGkZyUr4HyXhmXyb5ANip97OkfwTOJfnG3nPAV0j62zvVvpb0G5Jx6/oC75BcUG02Gfs3DcufkXS3bQK+HBHVLXrerhgQZmbWtK7YxWRmZkVwQJiZWSYHhJmZZXJAmJlZJgeEmZllckCYNUHSdkmLcv612aB3kg7NHaHTbHdSskuOmnUimyNiWEcXYdbefARh1kKSVkr6J0kvSPqzpI+n0w+V9Eg6Fv/Dkj6aTj9Y0n9K+kv678R0UWWSbk6va/CApL3T9pcruZbH85JmddBqWhfmgDBr2t55XUzn5jy2LiKOIfnl6o3ptP8L/CoijgXuAKan06cDj0XEUJKxkhan0wcDMyLiaGAt8MV0+lRgeLqcS0u1cmaF+JfUZk2QtDEiemVMXwmcEhEr0kER346IAyW9B/SPiG3p9FUR0VfSaqAyd9iHdBj2B9OLviDpu0D3iPiJpD8CG0mGVJgdERtLvKpmO/ERhFnrRIHbzZE7TtB2Pjw3eBrJ1Q9HAAtyRig1axcOCLPWOTfn/0+nt58iGUkW4HySARMhuSzkZdBwzez9Ci1UUjdgYETMA74L7AfschRjVkr+RGLWtL0lLcq5/8eIqP+q6/6Snic5CpiUTvsmydXdria50tuX0+nfAmZK+nuSI4XLSK6ElqUM+I80RARMTy8jatZufA7CrIXScxBVEfFeR9diVgruYjIzs0w+gjAzs0w+gjAzs0wOCDMzy+SAMDOzTA4IMzPL5IAwM7NM/w1I24kG2Aj76gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "\n",
        "f1 = list(map(np.mean,history.history['f1_score']))\n",
        "val_f1 = list(map(np.mean,history.history['val_f1_score'])) \n",
        "\n",
        "plt.ylim([0.326, 0.327])\n",
        "plt.plot(epochs, f1, '.', label='Training f1', color='lightgreen')\n",
        "plt.plot(epochs, val_f1, '-', label='Validation f1', color='lightblue')\n",
        "plt.title('Training and validation f1 score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1 score')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "RfTL30a9O37u",
        "outputId": "1185409a-b27b-4eec-80a7-76c01c1151c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pd.DataFrame(model.predict(X_test)).round(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2Pl-3a1RQ92",
        "outputId": "e0c30711-2578-49ce-f891-3081ccf4d4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "573/573 [==============================] - 1s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy\n",
        "1-len(y_pred.compare(pd.DataFrame(y_test)))/len(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D4_Lxwybh_3",
        "outputId": "d513144c-f084-42ee-9e30-bf8a4fe516d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9439869102808836"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/microsoft/FLAML.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "Uhj2ix8cQE3U",
        "outputId": "2e9f62cb-6f28-4810-fe1f-b99281799771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/microsoft/FLAML.git\n",
            "  Cloning https://github.com/microsoft/FLAML.git to /tmp/pip-req-build-75pa_2ap\n",
            "  Running command git clone -q https://github.com/microsoft/FLAML.git /tmp/pip-req-build-75pa_2ap\n",
            "Requirement already satisfied: NumPy>=1.17.0rc1 in /usr/local/lib/python3.8/dist-packages (from FLAML==1.1.0) (1.21.6)\n",
            "Requirement already satisfied: lightgbm>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from FLAML==1.1.0) (3.3.3)\n",
            "Requirement already satisfied: xgboost>=0.90 in /usr/local/lib/python3.8/dist-packages (from FLAML==1.1.0) (0.90)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from FLAML==1.1.0) (1.7.3)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from FLAML==1.1.0) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.8/dist-packages (from FLAML==1.1.0) (1.0.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from lightgbm>=2.3.1->FLAML==1.1.0) (0.38.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->FLAML==1.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->FLAML==1.1.0) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.4->FLAML==1.1.0) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24->FLAML==1.1.0) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24->FLAML==1.1.0) (3.1.0)\n",
            "Building wheels for collected packages: FLAML\n",
            "  Building wheel for FLAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for FLAML: filename=FLAML-1.1.0-py3-none-any.whl size=219759 sha256=2bf0327140a29f8032178f9285fb8a41be039392bd1a9ba62813873e087db2ce\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8i7e7b92/wheels/5c/1a/48/c07dfe482b630f96d7258700d361a971759465895f9dd768ee\n",
            "Successfully built FLAML\n",
            "Installing collected packages: FLAML\n",
            "  Attempting uninstall: FLAML\n",
            "    Found existing installation: FLAML 1.0.14\n",
            "    Uninstalling FLAML-1.0.14:\n",
            "      Successfully uninstalled FLAML-1.0.14\n",
            "Successfully installed FLAML-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X[78586:]\n",
        "y_test = y[78586:]\n",
        "X_data = X[:78586]\n",
        "y_data = y[:78586]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, shuffle=True,\n",
        "                                                      stratify=y_data, random_state=12)"
      ],
      "metadata": {
        "id": "EpfC6sq1UDqS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "JjIvdouPWySs",
        "outputId": "2c46b8c4-42de-4237-8e72-f2f9369256b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            ma_w     macd_w  macdsignal_w  macdhist_w      rsi_w          ad_w\n",
              "24597  12.200000   6.698147      6.332111    0.366036   1.320197   4081.215789\n",
              "22630   2.293333 -10.087122     -3.397035   -6.690088 -11.309699  -2331.369761\n",
              "14738 -13.830000  -2.690497     -2.569892   -0.120605   3.882245  -8472.948522\n",
              "50497 -34.236667 -10.165815     -6.295530   -3.870285  -2.723221  51012.290000\n",
              "21166   2.953333  -8.038478     -2.893477   -5.145001  -7.889032 -14234.133333\n",
              "...          ...        ...           ...         ...        ...           ...\n",
              "69815  -9.383333   2.328874      1.505066    0.823808   3.297338   3015.000000\n",
              "46214  -3.566667   4.536408      3.234773    1.301635  -3.286072  -8043.066667\n",
              "51354  12.936667  -1.640134      7.595263   -9.235396 -16.883311 -12450.397143\n",
              "70839  20.283333  -5.939089     -5.549629   -0.389460  -0.074745  42065.650000\n",
              "15024  -4.760000  12.823529      8.026058    4.797471  17.094208  20478.944444\n",
              "\n",
              "[62868 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f22cd76c-663b-4f42-8d15-98ca2cb94085\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ma_w</th>\n",
              "      <th>macd_w</th>\n",
              "      <th>macdsignal_w</th>\n",
              "      <th>macdhist_w</th>\n",
              "      <th>rsi_w</th>\n",
              "      <th>ad_w</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24597</th>\n",
              "      <td>12.200000</td>\n",
              "      <td>6.698147</td>\n",
              "      <td>6.332111</td>\n",
              "      <td>0.366036</td>\n",
              "      <td>1.320197</td>\n",
              "      <td>4081.215789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22630</th>\n",
              "      <td>2.293333</td>\n",
              "      <td>-10.087122</td>\n",
              "      <td>-3.397035</td>\n",
              "      <td>-6.690088</td>\n",
              "      <td>-11.309699</td>\n",
              "      <td>-2331.369761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14738</th>\n",
              "      <td>-13.830000</td>\n",
              "      <td>-2.690497</td>\n",
              "      <td>-2.569892</td>\n",
              "      <td>-0.120605</td>\n",
              "      <td>3.882245</td>\n",
              "      <td>-8472.948522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50497</th>\n",
              "      <td>-34.236667</td>\n",
              "      <td>-10.165815</td>\n",
              "      <td>-6.295530</td>\n",
              "      <td>-3.870285</td>\n",
              "      <td>-2.723221</td>\n",
              "      <td>51012.290000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21166</th>\n",
              "      <td>2.953333</td>\n",
              "      <td>-8.038478</td>\n",
              "      <td>-2.893477</td>\n",
              "      <td>-5.145001</td>\n",
              "      <td>-7.889032</td>\n",
              "      <td>-14234.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69815</th>\n",
              "      <td>-9.383333</td>\n",
              "      <td>2.328874</td>\n",
              "      <td>1.505066</td>\n",
              "      <td>0.823808</td>\n",
              "      <td>3.297338</td>\n",
              "      <td>3015.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46214</th>\n",
              "      <td>-3.566667</td>\n",
              "      <td>4.536408</td>\n",
              "      <td>3.234773</td>\n",
              "      <td>1.301635</td>\n",
              "      <td>-3.286072</td>\n",
              "      <td>-8043.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51354</th>\n",
              "      <td>12.936667</td>\n",
              "      <td>-1.640134</td>\n",
              "      <td>7.595263</td>\n",
              "      <td>-9.235396</td>\n",
              "      <td>-16.883311</td>\n",
              "      <td>-12450.397143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70839</th>\n",
              "      <td>20.283333</td>\n",
              "      <td>-5.939089</td>\n",
              "      <td>-5.549629</td>\n",
              "      <td>-0.389460</td>\n",
              "      <td>-0.074745</td>\n",
              "      <td>42065.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15024</th>\n",
              "      <td>-4.760000</td>\n",
              "      <td>12.823529</td>\n",
              "      <td>8.026058</td>\n",
              "      <td>4.797471</td>\n",
              "      <td>17.094208</td>\n",
              "      <td>20478.944444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62868 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f22cd76c-663b-4f42-8d15-98ca2cb94085')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f22cd76c-663b-4f42-8d15-98ca2cb94085 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f22cd76c-663b-4f42-8d15-98ca2cb94085');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y2e0jRZW0CJ",
        "outputId": "c8327a04-992a-4813-841b-6801c129bb02"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24597    H\n",
              "22630    H\n",
              "14738    H\n",
              "50497    B\n",
              "21166    H\n",
              "        ..\n",
              "69815    H\n",
              "46214    H\n",
              "51354    H\n",
              "70839    S\n",
              "15024    H\n",
              "Name: label, Length: 62868, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flaml import AutoML\n",
        "automl = AutoML()\n",
        "\n",
        "automl.fit(X_train, y_train,\n",
        "           task='classification',\n",
        "           X_val=X_val, y_val=y_val,\n",
        "           time_budget=60*10,\n",
        "           log_file_name='.automl.log',\n",
        "           estimator_list=['lgbm'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHEHUqYrL7xB",
        "outputId": "e2db7cfa-4cc0-487a-f964-2b886ea0d2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[flaml.automl: 12-11 13:12:14] {2599} INFO - task = classification\n",
            "INFO:flaml.automl:task = classification\n",
            "[flaml.automl: 12-11 13:12:14] {2601} INFO - Data split method: stratified\n",
            "INFO:flaml.automl:Data split method: stratified\n",
            "[flaml.automl: 12-11 13:12:14] {2604} INFO - Evaluation method: holdout\n",
            "INFO:flaml.automl:Evaluation method: holdout\n",
            "[flaml.automl: 12-11 13:12:14] {2726} INFO - Minimizing error metric: log_loss\n",
            "INFO:flaml.automl:Minimizing error metric: log_loss\n",
            "[flaml.automl: 12-11 13:12:14] {2870} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "INFO:flaml.automl:List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl: 12-11 13:12:14] {3166} INFO - iteration 0, current learner lgbm\n",
            "INFO:flaml.automl:iteration 0, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:14] {3296} INFO - Estimated sufficient time budget=7263s. Estimated necessary time budget=7s.\n",
            "INFO:flaml.automl:Estimated sufficient time budget=7263s. Estimated necessary time budget=7s.\n",
            "[flaml.automl: 12-11 13:12:14] {3343} INFO -  at 0.6s,\testimator lgbm's best error=0.1848,\tbest estimator lgbm's best error=0.1848\n",
            "INFO:flaml.automl: at 0.6s,\testimator lgbm's best error=0.1848,\tbest estimator lgbm's best error=0.1848\n",
            "[flaml.automl: 12-11 13:12:14] {3166} INFO - iteration 1, current learner lgbm\n",
            "INFO:flaml.automl:iteration 1, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:14] {3343} INFO -  at 0.7s,\testimator lgbm's best error=0.1848,\tbest estimator lgbm's best error=0.1848\n",
            "INFO:flaml.automl: at 0.7s,\testimator lgbm's best error=0.1848,\tbest estimator lgbm's best error=0.1848\n",
            "[flaml.automl: 12-11 13:12:14] {3166} INFO - iteration 2, current learner lgbm\n",
            "INFO:flaml.automl:iteration 2, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:14] {3343} INFO -  at 0.8s,\testimator lgbm's best error=0.1764,\tbest estimator lgbm's best error=0.1764\n",
            "INFO:flaml.automl: at 0.8s,\testimator lgbm's best error=0.1764,\tbest estimator lgbm's best error=0.1764\n",
            "[flaml.automl: 12-11 13:12:14] {3166} INFO - iteration 3, current learner lgbm\n",
            "INFO:flaml.automl:iteration 3, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:14] {3343} INFO -  at 0.8s,\testimator lgbm's best error=0.1764,\tbest estimator lgbm's best error=0.1764\n",
            "INFO:flaml.automl: at 0.8s,\testimator lgbm's best error=0.1764,\tbest estimator lgbm's best error=0.1764\n",
            "[flaml.automl: 12-11 13:12:14] {3166} INFO - iteration 4, current learner lgbm\n",
            "INFO:flaml.automl:iteration 4, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:14] {3343} INFO -  at 0.9s,\testimator lgbm's best error=0.1764,\tbest estimator lgbm's best error=0.1764\n",
            "INFO:flaml.automl: at 0.9s,\testimator lgbm's best error=0.1764,\tbest estimator lgbm's best error=0.1764\n",
            "[flaml.automl: 12-11 13:12:14] {3166} INFO - iteration 5, current learner lgbm\n",
            "INFO:flaml.automl:iteration 5, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:14] {3343} INFO -  at 0.9s,\testimator lgbm's best error=0.1764,\tbest estimator lgbm's best error=0.1764\n",
            "INFO:flaml.automl: at 0.9s,\testimator lgbm's best error=0.1764,\tbest estimator lgbm's best error=0.1764\n",
            "[flaml.automl: 12-11 13:12:14] {3166} INFO - iteration 6, current learner lgbm\n",
            "INFO:flaml.automl:iteration 6, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:14] {3343} INFO -  at 1.1s,\testimator lgbm's best error=0.1750,\tbest estimator lgbm's best error=0.1750\n",
            "INFO:flaml.automl: at 1.1s,\testimator lgbm's best error=0.1750,\tbest estimator lgbm's best error=0.1750\n",
            "[flaml.automl: 12-11 13:12:14] {3166} INFO - iteration 7, current learner lgbm\n",
            "INFO:flaml.automl:iteration 7, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:15] {3343} INFO -  at 1.2s,\testimator lgbm's best error=0.1746,\tbest estimator lgbm's best error=0.1746\n",
            "INFO:flaml.automl: at 1.2s,\testimator lgbm's best error=0.1746,\tbest estimator lgbm's best error=0.1746\n",
            "[flaml.automl: 12-11 13:12:15] {3166} INFO - iteration 8, current learner lgbm\n",
            "INFO:flaml.automl:iteration 8, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:15] {3343} INFO -  at 1.5s,\testimator lgbm's best error=0.1708,\tbest estimator lgbm's best error=0.1708\n",
            "INFO:flaml.automl: at 1.5s,\testimator lgbm's best error=0.1708,\tbest estimator lgbm's best error=0.1708\n",
            "[flaml.automl: 12-11 13:12:15] {3166} INFO - iteration 9, current learner lgbm\n",
            "INFO:flaml.automl:iteration 9, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:15] {3343} INFO -  at 1.6s,\testimator lgbm's best error=0.1708,\tbest estimator lgbm's best error=0.1708\n",
            "INFO:flaml.automl: at 1.6s,\testimator lgbm's best error=0.1708,\tbest estimator lgbm's best error=0.1708\n",
            "[flaml.automl: 12-11 13:12:15] {3166} INFO - iteration 10, current learner lgbm\n",
            "INFO:flaml.automl:iteration 10, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:16] {3343} INFO -  at 2.4s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "INFO:flaml.automl: at 2.4s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "[flaml.automl: 12-11 13:12:16] {3166} INFO - iteration 11, current learner lgbm\n",
            "INFO:flaml.automl:iteration 11, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:17] {3343} INFO -  at 3.4s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "INFO:flaml.automl: at 3.4s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "[flaml.automl: 12-11 13:12:17] {3166} INFO - iteration 12, current learner lgbm\n",
            "INFO:flaml.automl:iteration 12, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:17] {3343} INFO -  at 4.1s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "INFO:flaml.automl: at 4.1s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "[flaml.automl: 12-11 13:12:17] {3166} INFO - iteration 13, current learner lgbm\n",
            "INFO:flaml.automl:iteration 13, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:18] {3343} INFO -  at 5.0s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "INFO:flaml.automl: at 5.0s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "[flaml.automl: 12-11 13:12:18] {3166} INFO - iteration 14, current learner lgbm\n",
            "INFO:flaml.automl:iteration 14, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:19] {3343} INFO -  at 5.8s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "INFO:flaml.automl: at 5.8s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "[flaml.automl: 12-11 13:12:19] {3166} INFO - iteration 15, current learner lgbm\n",
            "INFO:flaml.automl:iteration 15, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:20] {3343} INFO -  at 6.8s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "INFO:flaml.automl: at 6.8s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "[flaml.automl: 12-11 13:12:20] {3166} INFO - iteration 16, current learner lgbm\n",
            "INFO:flaml.automl:iteration 16, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:21] {3343} INFO -  at 7.5s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "INFO:flaml.automl: at 7.5s,\testimator lgbm's best error=0.1671,\tbest estimator lgbm's best error=0.1671\n",
            "[flaml.automl: 12-11 13:12:21] {3166} INFO - iteration 17, current learner lgbm\n",
            "INFO:flaml.automl:iteration 17, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:22] {3343} INFO -  at 8.5s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 8.5s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:22] {3166} INFO - iteration 18, current learner lgbm\n",
            "INFO:flaml.automl:iteration 18, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:23] {3343} INFO -  at 9.3s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 9.3s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:23] {3166} INFO - iteration 19, current learner lgbm\n",
            "INFO:flaml.automl:iteration 19, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:23] {3343} INFO -  at 9.8s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 9.8s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:23] {3166} INFO - iteration 20, current learner lgbm\n",
            "INFO:flaml.automl:iteration 20, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:26] {3343} INFO -  at 12.5s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 12.5s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:26] {3166} INFO - iteration 21, current learner lgbm\n",
            "INFO:flaml.automl:iteration 21, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:28] {3343} INFO -  at 14.5s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 14.5s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:28] {3166} INFO - iteration 22, current learner lgbm\n",
            "INFO:flaml.automl:iteration 22, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:28] {3343} INFO -  at 15.2s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 15.2s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:28] {3166} INFO - iteration 23, current learner lgbm\n",
            "INFO:flaml.automl:iteration 23, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:29] {3343} INFO -  at 16.2s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 16.2s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:29] {3166} INFO - iteration 24, current learner lgbm\n",
            "INFO:flaml.automl:iteration 24, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:31] {3343} INFO -  at 17.4s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 17.4s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:31] {3166} INFO - iteration 25, current learner lgbm\n",
            "INFO:flaml.automl:iteration 25, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:34] {3343} INFO -  at 20.4s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 20.4s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:34] {3166} INFO - iteration 26, current learner lgbm\n",
            "INFO:flaml.automl:iteration 26, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:34] {3343} INFO -  at 21.1s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 21.1s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:34] {3166} INFO - iteration 27, current learner lgbm\n",
            "INFO:flaml.automl:iteration 27, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:37] {3343} INFO -  at 23.8s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 23.8s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:37] {3166} INFO - iteration 28, current learner lgbm\n",
            "INFO:flaml.automl:iteration 28, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:38] {3343} INFO -  at 24.3s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 24.3s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:38] {3166} INFO - iteration 29, current learner lgbm\n",
            "INFO:flaml.automl:iteration 29, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:38] {3343} INFO -  at 24.8s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 24.8s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:38] {3166} INFO - iteration 30, current learner lgbm\n",
            "INFO:flaml.automl:iteration 30, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:44] {3343} INFO -  at 31.2s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 31.2s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:44] {3166} INFO - iteration 31, current learner lgbm\n",
            "INFO:flaml.automl:iteration 31, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:46] {3343} INFO -  at 32.9s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "INFO:flaml.automl: at 32.9s,\testimator lgbm's best error=0.1658,\tbest estimator lgbm's best error=0.1658\n",
            "[flaml.automl: 12-11 13:12:46] {3166} INFO - iteration 32, current learner lgbm\n",
            "INFO:flaml.automl:iteration 32, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:58] {3343} INFO -  at 44.3s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 44.3s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:12:58] {3166} INFO - iteration 33, current learner lgbm\n",
            "INFO:flaml.automl:iteration 33, current learner lgbm\n",
            "[flaml.automl: 12-11 13:12:59] {3343} INFO -  at 45.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 45.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:12:59] {3166} INFO - iteration 34, current learner lgbm\n",
            "INFO:flaml.automl:iteration 34, current learner lgbm\n",
            "[flaml.automl: 12-11 13:13:01] {3343} INFO -  at 47.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 47.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:13:01] {3166} INFO - iteration 35, current learner lgbm\n",
            "INFO:flaml.automl:iteration 35, current learner lgbm\n",
            "[flaml.automl: 12-11 13:13:11] {3343} INFO -  at 58.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 58.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:13:11] {3166} INFO - iteration 36, current learner lgbm\n",
            "INFO:flaml.automl:iteration 36, current learner lgbm\n",
            "[flaml.automl: 12-11 13:13:12] {3343} INFO -  at 58.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 58.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:13:12] {3166} INFO - iteration 37, current learner lgbm\n",
            "INFO:flaml.automl:iteration 37, current learner lgbm\n",
            "[flaml.automl: 12-11 13:13:17] {3343} INFO -  at 63.2s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 63.2s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:13:17] {3166} INFO - iteration 38, current learner lgbm\n",
            "INFO:flaml.automl:iteration 38, current learner lgbm\n",
            "[flaml.automl: 12-11 13:13:17] {3343} INFO -  at 64.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 64.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:13:17] {3166} INFO - iteration 39, current learner lgbm\n",
            "INFO:flaml.automl:iteration 39, current learner lgbm\n",
            "[flaml.automl: 12-11 13:13:44] {3343} INFO -  at 91.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 91.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:13:44] {3166} INFO - iteration 40, current learner lgbm\n",
            "INFO:flaml.automl:iteration 40, current learner lgbm\n",
            "[flaml.automl: 12-11 13:13:45] {3343} INFO -  at 91.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 91.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:13:45] {3166} INFO - iteration 41, current learner lgbm\n",
            "INFO:flaml.automl:iteration 41, current learner lgbm\n",
            "[flaml.automl: 12-11 13:13:45] {3343} INFO -  at 92.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 92.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:13:45] {3166} INFO - iteration 42, current learner lgbm\n",
            "INFO:flaml.automl:iteration 42, current learner lgbm\n",
            "[flaml.automl: 12-11 13:14:04] {3343} INFO -  at 111.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 111.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:14:04] {3166} INFO - iteration 43, current learner lgbm\n",
            "INFO:flaml.automl:iteration 43, current learner lgbm\n",
            "[flaml.automl: 12-11 13:14:15] {3343} INFO -  at 121.4s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 121.4s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:14:15] {3166} INFO - iteration 44, current learner lgbm\n",
            "INFO:flaml.automl:iteration 44, current learner lgbm\n",
            "[flaml.automl: 12-11 13:14:17] {3343} INFO -  at 123.7s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 123.7s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:14:17] {3166} INFO - iteration 45, current learner lgbm\n",
            "INFO:flaml.automl:iteration 45, current learner lgbm\n",
            "[flaml.automl: 12-11 13:14:18] {3343} INFO -  at 124.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 124.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:14:18] {3166} INFO - iteration 46, current learner lgbm\n",
            "INFO:flaml.automl:iteration 46, current learner lgbm\n",
            "[flaml.automl: 12-11 13:14:37] {3343} INFO -  at 143.7s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 143.7s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:14:37] {3166} INFO - iteration 47, current learner lgbm\n",
            "INFO:flaml.automl:iteration 47, current learner lgbm\n",
            "[flaml.automl: 12-11 13:14:37] {3343} INFO -  at 144.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 144.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:14:37] {3166} INFO - iteration 48, current learner lgbm\n",
            "INFO:flaml.automl:iteration 48, current learner lgbm\n",
            "[flaml.automl: 12-11 13:14:59] {3343} INFO -  at 165.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 165.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:14:59] {3166} INFO - iteration 49, current learner lgbm\n",
            "INFO:flaml.automl:iteration 49, current learner lgbm\n",
            "[flaml.automl: 12-11 13:15:00] {3343} INFO -  at 166.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 166.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:15:00] {3166} INFO - iteration 50, current learner lgbm\n",
            "INFO:flaml.automl:iteration 50, current learner lgbm\n",
            "[flaml.automl: 12-11 13:15:47] {3343} INFO -  at 213.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 213.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:15:47] {3166} INFO - iteration 51, current learner lgbm\n",
            "INFO:flaml.automl:iteration 51, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:04] {3343} INFO -  at 230.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 230.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:04] {3166} INFO - iteration 52, current learner lgbm\n",
            "INFO:flaml.automl:iteration 52, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:05] {3343} INFO -  at 231.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 231.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:05] {3166} INFO - iteration 53, current learner lgbm\n",
            "INFO:flaml.automl:iteration 53, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:20] {3343} INFO -  at 246.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 246.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:20] {3166} INFO - iteration 54, current learner lgbm\n",
            "INFO:flaml.automl:iteration 54, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:21] {3343} INFO -  at 247.4s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 247.4s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:21] {3166} INFO - iteration 55, current learner lgbm\n",
            "INFO:flaml.automl:iteration 55, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:24] {3343} INFO -  at 250.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 250.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:24] {3166} INFO - iteration 56, current learner lgbm\n",
            "INFO:flaml.automl:iteration 56, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:26] {3343} INFO -  at 252.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 252.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:26] {3166} INFO - iteration 57, current learner lgbm\n",
            "INFO:flaml.automl:iteration 57, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:28] {3343} INFO -  at 254.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 254.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:28] {3166} INFO - iteration 58, current learner lgbm\n",
            "INFO:flaml.automl:iteration 58, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:30] {3343} INFO -  at 256.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 256.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:30] {3166} INFO - iteration 59, current learner lgbm\n",
            "INFO:flaml.automl:iteration 59, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:31] {3343} INFO -  at 257.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 257.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:31] {3166} INFO - iteration 60, current learner lgbm\n",
            "INFO:flaml.automl:iteration 60, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:39] {3343} INFO -  at 266.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 266.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:39] {3166} INFO - iteration 61, current learner lgbm\n",
            "INFO:flaml.automl:iteration 61, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:53] {3343} INFO -  at 279.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 279.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:53] {3166} INFO - iteration 62, current learner lgbm\n",
            "INFO:flaml.automl:iteration 62, current learner lgbm\n",
            "[flaml.automl: 12-11 13:16:53] {3343} INFO -  at 280.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 280.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:16:53] {3166} INFO - iteration 63, current learner lgbm\n",
            "INFO:flaml.automl:iteration 63, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:06] {3343} INFO -  at 293.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 293.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:06] {3166} INFO - iteration 64, current learner lgbm\n",
            "INFO:flaml.automl:iteration 64, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:07] {3343} INFO -  at 294.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 294.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:07] {3166} INFO - iteration 65, current learner lgbm\n",
            "INFO:flaml.automl:iteration 65, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:09] {3343} INFO -  at 296.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 296.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:09] {3166} INFO - iteration 66, current learner lgbm\n",
            "INFO:flaml.automl:iteration 66, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:12] {3343} INFO -  at 299.2s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 299.2s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:12] {3166} INFO - iteration 67, current learner lgbm\n",
            "INFO:flaml.automl:iteration 67, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:14] {3343} INFO -  at 300.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 300.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:14] {3166} INFO - iteration 68, current learner lgbm\n",
            "INFO:flaml.automl:iteration 68, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:18] {3343} INFO -  at 305.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 305.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:18] {3166} INFO - iteration 69, current learner lgbm\n",
            "INFO:flaml.automl:iteration 69, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:32] {3343} INFO -  at 319.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 319.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:32] {3166} INFO - iteration 70, current learner lgbm\n",
            "INFO:flaml.automl:iteration 70, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:34] {3343} INFO -  at 320.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 320.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:34] {3166} INFO - iteration 71, current learner lgbm\n",
            "INFO:flaml.automl:iteration 71, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:37] {3343} INFO -  at 323.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 323.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:37] {3166} INFO - iteration 72, current learner lgbm\n",
            "INFO:flaml.automl:iteration 72, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:39] {3343} INFO -  at 325.3s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 325.3s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:39] {3166} INFO - iteration 73, current learner lgbm\n",
            "INFO:flaml.automl:iteration 73, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:39] {3343} INFO -  at 325.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 325.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:39] {3166} INFO - iteration 74, current learner lgbm\n",
            "INFO:flaml.automl:iteration 74, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:52] {3343} INFO -  at 338.4s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 338.4s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:52] {3166} INFO - iteration 75, current learner lgbm\n",
            "INFO:flaml.automl:iteration 75, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:57] {3343} INFO -  at 344.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 344.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:57] {3166} INFO - iteration 76, current learner lgbm\n",
            "INFO:flaml.automl:iteration 76, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:58] {3343} INFO -  at 344.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 344.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:58] {3166} INFO - iteration 77, current learner lgbm\n",
            "INFO:flaml.automl:iteration 77, current learner lgbm\n",
            "[flaml.automl: 12-11 13:17:59] {3343} INFO -  at 345.3s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 345.3s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:17:59] {3166} INFO - iteration 78, current learner lgbm\n",
            "INFO:flaml.automl:iteration 78, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:15] {3343} INFO -  at 361.7s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 361.7s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:15] {3166} INFO - iteration 79, current learner lgbm\n",
            "INFO:flaml.automl:iteration 79, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:23] {3343} INFO -  at 369.3s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 369.3s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:23] {3166} INFO - iteration 80, current learner lgbm\n",
            "INFO:flaml.automl:iteration 80, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:23] {3343} INFO -  at 370.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 370.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:23] {3166} INFO - iteration 81, current learner lgbm\n",
            "INFO:flaml.automl:iteration 81, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:38] {3343} INFO -  at 385.2s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 385.2s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:38] {3166} INFO - iteration 82, current learner lgbm\n",
            "INFO:flaml.automl:iteration 82, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:39] {3343} INFO -  at 385.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 385.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:39] {3166} INFO - iteration 83, current learner lgbm\n",
            "INFO:flaml.automl:iteration 83, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:41] {3343} INFO -  at 388.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 388.0s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:41] {3166} INFO - iteration 84, current learner lgbm\n",
            "INFO:flaml.automl:iteration 84, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:44] {3343} INFO -  at 390.2s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 390.2s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:44] {3166} INFO - iteration 85, current learner lgbm\n",
            "INFO:flaml.automl:iteration 85, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:44] {3343} INFO -  at 391.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 391.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:44] {3166} INFO - iteration 86, current learner lgbm\n",
            "INFO:flaml.automl:iteration 86, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:52] {3343} INFO -  at 398.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 398.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:52] {3166} INFO - iteration 87, current learner lgbm\n",
            "INFO:flaml.automl:iteration 87, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:55] {3343} INFO -  at 402.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 402.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:55] {3166} INFO - iteration 88, current learner lgbm\n",
            "INFO:flaml.automl:iteration 88, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:57] {3343} INFO -  at 403.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 403.5s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:57] {3166} INFO - iteration 89, current learner lgbm\n",
            "INFO:flaml.automl:iteration 89, current learner lgbm\n",
            "[flaml.automl: 12-11 13:18:58] {3343} INFO -  at 404.7s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 404.7s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:18:58] {3166} INFO - iteration 90, current learner lgbm\n",
            "INFO:flaml.automl:iteration 90, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:01] {3343} INFO -  at 407.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 407.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:19:01] {3166} INFO - iteration 91, current learner lgbm\n",
            "INFO:flaml.automl:iteration 91, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:03] {3343} INFO -  at 409.3s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 409.3s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:19:03] {3166} INFO - iteration 92, current learner lgbm\n",
            "INFO:flaml.automl:iteration 92, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:06] {3343} INFO -  at 413.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 413.1s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:19:06] {3166} INFO - iteration 93, current learner lgbm\n",
            "INFO:flaml.automl:iteration 93, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:07] {3343} INFO -  at 413.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 413.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:19:07] {3166} INFO - iteration 94, current learner lgbm\n",
            "INFO:flaml.automl:iteration 94, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:14] {3343} INFO -  at 420.4s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 420.4s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:19:14] {3166} INFO - iteration 95, current learner lgbm\n",
            "INFO:flaml.automl:iteration 95, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:19] {3343} INFO -  at 425.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 425.9s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:19:19] {3166} INFO - iteration 96, current learner lgbm\n",
            "INFO:flaml.automl:iteration 96, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:21] {3343} INFO -  at 427.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 427.8s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:19:21] {3166} INFO - iteration 97, current learner lgbm\n",
            "INFO:flaml.automl:iteration 97, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:23] {3343} INFO -  at 429.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "INFO:flaml.automl: at 429.6s,\testimator lgbm's best error=0.1656,\tbest estimator lgbm's best error=0.1656\n",
            "[flaml.automl: 12-11 13:19:23] {3166} INFO - iteration 98, current learner lgbm\n",
            "INFO:flaml.automl:iteration 98, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:26] {3343} INFO -  at 432.5s,\testimator lgbm's best error=0.1655,\tbest estimator lgbm's best error=0.1655\n",
            "INFO:flaml.automl: at 432.5s,\testimator lgbm's best error=0.1655,\tbest estimator lgbm's best error=0.1655\n",
            "[flaml.automl: 12-11 13:19:26] {3166} INFO - iteration 99, current learner lgbm\n",
            "INFO:flaml.automl:iteration 99, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:28] {3343} INFO -  at 434.9s,\testimator lgbm's best error=0.1655,\tbest estimator lgbm's best error=0.1655\n",
            "INFO:flaml.automl: at 434.9s,\testimator lgbm's best error=0.1655,\tbest estimator lgbm's best error=0.1655\n",
            "[flaml.automl: 12-11 13:19:28] {3166} INFO - iteration 100, current learner lgbm\n",
            "INFO:flaml.automl:iteration 100, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:31] {3343} INFO -  at 437.8s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 437.8s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:19:31] {3166} INFO - iteration 101, current learner lgbm\n",
            "INFO:flaml.automl:iteration 101, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:39] {3343} INFO -  at 445.2s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 445.2s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:19:39] {3166} INFO - iteration 102, current learner lgbm\n",
            "INFO:flaml.automl:iteration 102, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:40] {3343} INFO -  at 446.3s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 446.3s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:19:40] {3166} INFO - iteration 103, current learner lgbm\n",
            "INFO:flaml.automl:iteration 103, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:47] {3343} INFO -  at 453.9s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 453.9s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:19:47] {3166} INFO - iteration 104, current learner lgbm\n",
            "INFO:flaml.automl:iteration 104, current learner lgbm\n",
            "[flaml.automl: 12-11 13:19:48] {3343} INFO -  at 455.1s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 455.1s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:19:48] {3166} INFO - iteration 105, current learner lgbm\n",
            "INFO:flaml.automl:iteration 105, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:00] {3343} INFO -  at 466.5s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 466.5s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:00] {3166} INFO - iteration 106, current learner lgbm\n",
            "INFO:flaml.automl:iteration 106, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:01] {3343} INFO -  at 467.9s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 467.9s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:01] {3166} INFO - iteration 107, current learner lgbm\n",
            "INFO:flaml.automl:iteration 107, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:02] {3343} INFO -  at 468.7s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 468.7s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:02] {3166} INFO - iteration 108, current learner lgbm\n",
            "INFO:flaml.automl:iteration 108, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:17] {3343} INFO -  at 484.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 484.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:17] {3166} INFO - iteration 109, current learner lgbm\n",
            "INFO:flaml.automl:iteration 109, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:19] {3343} INFO -  at 485.8s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 485.8s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:19] {3166} INFO - iteration 110, current learner lgbm\n",
            "INFO:flaml.automl:iteration 110, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:24] {3343} INFO -  at 490.8s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 490.8s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:24] {3166} INFO - iteration 111, current learner lgbm\n",
            "INFO:flaml.automl:iteration 111, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:39] {3343} INFO -  at 505.8s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 505.8s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:39] {3166} INFO - iteration 112, current learner lgbm\n",
            "INFO:flaml.automl:iteration 112, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:40] {3343} INFO -  at 506.5s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 506.5s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:40] {3166} INFO - iteration 113, current learner lgbm\n",
            "INFO:flaml.automl:iteration 113, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:42] {3343} INFO -  at 509.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 509.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:42] {3166} INFO - iteration 114, current learner lgbm\n",
            "INFO:flaml.automl:iteration 114, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:45] {3343} INFO -  at 512.1s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 512.1s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:45] {3166} INFO - iteration 115, current learner lgbm\n",
            "INFO:flaml.automl:iteration 115, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:46] {3343} INFO -  at 513.2s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 513.2s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:46] {3166} INFO - iteration 116, current learner lgbm\n",
            "INFO:flaml.automl:iteration 116, current learner lgbm\n",
            "[flaml.automl: 12-11 13:20:57] {3343} INFO -  at 524.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 524.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:20:57] {3166} INFO - iteration 117, current learner lgbm\n",
            "INFO:flaml.automl:iteration 117, current learner lgbm\n",
            "[flaml.automl: 12-11 13:21:18] {3343} INFO -  at 544.8s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 544.8s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:21:18] {3166} INFO - iteration 118, current learner lgbm\n",
            "INFO:flaml.automl:iteration 118, current learner lgbm\n",
            "[flaml.automl: 12-11 13:21:19] {3343} INFO -  at 545.4s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 545.4s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:21:19] {3166} INFO - iteration 119, current learner lgbm\n",
            "INFO:flaml.automl:iteration 119, current learner lgbm\n",
            "[flaml.automl: 12-11 13:21:22] {3343} INFO -  at 548.5s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 548.5s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:21:22] {3166} INFO - iteration 120, current learner lgbm\n",
            "INFO:flaml.automl:iteration 120, current learner lgbm\n",
            "[flaml.automl: 12-11 13:21:24] {3343} INFO -  at 550.7s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 550.7s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:21:24] {3166} INFO - iteration 121, current learner lgbm\n",
            "INFO:flaml.automl:iteration 121, current learner lgbm\n",
            "[flaml.automl: 12-11 13:21:30] {3343} INFO -  at 556.3s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 556.3s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:21:30] {3166} INFO - iteration 122, current learner lgbm\n",
            "INFO:flaml.automl:iteration 122, current learner lgbm\n",
            "[flaml.automl: 12-11 13:21:31] {3343} INFO -  at 558.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 558.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:21:31] {3166} INFO - iteration 123, current learner lgbm\n",
            "INFO:flaml.automl:iteration 123, current learner lgbm\n",
            "[flaml.automl: 12-11 13:21:34] {3343} INFO -  at 560.9s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 560.9s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:21:34] {3166} INFO - iteration 124, current learner lgbm\n",
            "INFO:flaml.automl:iteration 124, current learner lgbm\n",
            "[flaml.automl: 12-11 13:21:38] {3343} INFO -  at 565.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 565.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:21:38] {3166} INFO - iteration 125, current learner lgbm\n",
            "INFO:flaml.automl:iteration 125, current learner lgbm\n",
            "[flaml.automl: 12-11 13:21:40] {3343} INFO -  at 567.2s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 567.2s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:21:40] {3166} INFO - iteration 126, current learner lgbm\n",
            "INFO:flaml.automl:iteration 126, current learner lgbm\n",
            "[flaml.automl: 12-11 13:21:45] {3343} INFO -  at 572.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 572.0s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:21:45] {3166} INFO - iteration 127, current learner lgbm\n",
            "INFO:flaml.automl:iteration 127, current learner lgbm\n",
            "[flaml.automl: 12-11 13:22:06] {3343} INFO -  at 593.1s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 593.1s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:22:06] {3166} INFO - iteration 128, current learner lgbm\n",
            "INFO:flaml.automl:iteration 128, current learner lgbm\n",
            "[flaml.automl: 12-11 13:22:07] {3343} INFO -  at 593.7s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 593.7s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:22:07] {3166} INFO - iteration 129, current learner lgbm\n",
            "INFO:flaml.automl:iteration 129, current learner lgbm\n",
            "[flaml.automl: 12-11 13:22:10] {3343} INFO -  at 596.4s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 596.4s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:22:10] {3166} INFO - iteration 130, current learner lgbm\n",
            "INFO:flaml.automl:iteration 130, current learner lgbm\n",
            "[flaml.automl: 12-11 13:22:12] {3343} INFO -  at 599.1s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "INFO:flaml.automl: at 599.1s,\testimator lgbm's best error=0.1654,\tbest estimator lgbm's best error=0.1654\n",
            "[flaml.automl: 12-11 13:22:12] {3459} INFO - selected model: LGBMClassifier(colsample_bytree=0.9930690838565842,\n",
            "               learning_rate=0.023685869156657772, max_bin=511,\n",
            "               min_child_samples=14, n_estimators=124, num_leaves=34,\n",
            "               reg_alpha=0.0014141288873710895, reg_lambda=1.9806480337540229,\n",
            "               verbose=-1)\n",
            "INFO:flaml.automl:selected model: LGBMClassifier(colsample_bytree=0.9930690838565842,\n",
            "               learning_rate=0.023685869156657772, max_bin=511,\n",
            "               min_child_samples=14, n_estimators=124, num_leaves=34,\n",
            "               reg_alpha=0.0014141288873710895, reg_lambda=1.9806480337540229,\n",
            "               verbose=-1)\n",
            "[flaml.automl: 12-11 13:22:12] {2901} INFO - fit succeeded\n",
            "INFO:flaml.automl:fit succeeded\n",
            "[flaml.automl: 12-11 13:22:12] {2902} INFO - Time taken to find the best model: 437.78077721595764\n",
            "INFO:flaml.automl:Time taken to find the best model: 437.78077721595764\n",
            "[flaml.automl: 12-11 13:22:12] {2913} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n",
            "WARNING:flaml.automl:Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('.automl.pkl', 'wb') as f:\n",
        "    pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)\n",
        "with open('.automl.pkl', 'rb') as f:\n",
        "    automl = pickle.load(f)"
      ],
      "metadata": {
        "id": "Hl43Eh8EUP-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best ML leaner:', automl.best_estimator)\n",
        "print('Best hyperparmeter config:', automl.best_config)\n",
        "print('Best loss on validation data: {0:.4g}'.format(automl.best_loss))\n",
        "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwrSPE5uUg4l",
        "outputId": "b5c543f4-b340-4c41-97bc-544d14c89b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best ML leaner: lgbm\n",
            "Best hyperparmeter config: {'n_estimators': 124, 'num_leaves': 34, 'min_child_samples': 14, 'learning_rate': 0.023685869156657772, 'log_max_bin': 9, 'colsample_bytree': 0.9930690838565842, 'reg_alpha': 0.0014141288873710895, 'reg_lambda': 1.9806480337540229, 'FLAML_sample_size': 62868}\n",
            "Best loss on validation data: 0.1654\n",
            "Training duration of best run: 2.831 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = automl.predict(X_test)\n",
        "print('Predicted labels', y_pred)\n",
        "print('True labels', y_test)\n",
        "y_pred_proba = automl.predict_proba(X_test)[:,1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOfxKq_gUkC8",
        "outputId": "c368f2f3-95c0-42a0-a12a-a40095e86837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels ['H' 'H' 'H' ... 'H' 'H' 'H']\n",
            "True labels 78631    B\n",
            "78632    H\n",
            "78633    H\n",
            "78634    H\n",
            "78635    H\n",
            "        ..\n",
            "96961    H\n",
            "96962    H\n",
            "96963    H\n",
            "96964    H\n",
            "96965    H\n",
            "Name: label, Length: 18335, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_pred).unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtGbONNuXqTb",
        "outputId": "e2f389da-ea33-42f1-d2da-12176e3e5c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['H', 'S', 'B'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' compute different metric values on testing dataset'''\n",
        "from flaml.ml import sklearn_metric_loss_score\n",
        "print('accuracy', '=', 1 - sklearn_metric_loss_score('accuracy', y_pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W_ZF7ovUmA1",
        "outputId": "f3638845-20f3-4a35-d315-c6ad95bfb21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 0.9435505863103354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flaml.data import get_output_from_log\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = \\\n",
        "    get_output_from_log(filename='.automl.log', time_budget=60)\n",
        "\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Wall Clock Time (s)\")\n",
        "plt.ylabel(\"Validation loss\")\n",
        "plt.step(time_history, np.array(best_valid_loss_history), where=\"post\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "f-PgKaeIUoZX",
        "outputId": "d667866e-1e47-4bd1-935c-140657ef012e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(automl.feature_importances_, index=automl.feature_names_in_).sort_values(ascending=False).plot.bar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "LAlz9LoVUtsi",
        "outputId": "aa568b53-78b6-45e8-9675-9938b1310000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f824f1481f0>"
            ]
          },
          "metadata": {},
          "execution_count": 142
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAE1CAYAAAALcjBQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYFUlEQVR4nO3df7RlZX3f8ffHAQ0illGmVPnhIB3NGmMEHJFU/BWWiJgWTVMjTZEa0qENpGpNs8D+wOgyuqLRpVZpUYkYLYRGXU6RSiZoVKLoDD8Ehh+LCUKZEWEMamxUDObbP/a+4Yh3Zu7cOffsc8/zfq111jnnOfuc893r3PO5z3n23s9OVSFJasMjhi5AkjQ5hr4kNcTQl6SGGPqS1BBDX5IaYuhLUkP2GbqAXTnooINq9erVQ5chScvKNddc862qWjXfY1Md+qtXr2bz5s1DlyFJy0qSu3b2mMM7ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIZM9cFZi3Xxddsm+n6nHn3oRN9PkhbLnr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSG7Df0khyX5XJKbk2xJ8pq+/Y1Jtie5vr+cPPKcc5NsTXJbkhePtJ/Ut21Ncs7SrJIkaWcWcuasB4HXV9W1SQ4ArkmysX/sXVX1jtGFk6wFXgk8DXgi8GdJntI//D7gRcA2YFOSDVV18zhWRJK0e7sN/aq6B7inv/29JLcAh+ziKacAl1TVA8DXk2wFju0f21pVdwAkuaRf1tCXpAnZozH9JKuBo4Gv9E1nJ7khyYVJVvZthwB3jzxtW9+2s/aHv8f6JJuTbN6xY8eelCdJ2o0Fh36SxwAfB15bVX8NnA8cCRxF90vgD8ZRUFVdUFXrqmrdqlWrxvGSkqTeQsb0SbIvXeB/rKo+AVBV9448/gHgsv7uduCwkacf2rexi3ZJ0gQsZO+dAB8Cbqmqd460P2FksZcDN/W3NwCvTPKoJEcAa4CvApuANUmOSPJIuo29G8azGpKkhVhIT/85wGnAjUmu79veAJya5CiggDuBMwGqakuSS+k20D4InFVVPwZIcjZwBbACuLCqtoxxXSRJu7GQvXeuAjLPQ5fv4jlvAd4yT/vlu3qeJGlpeUSuJDXE0Jekhhj6ktQQQ1+SGmLoS1JDFnRwlqbLxddtm+j7nXr0oRN9P0lLx56+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGeLpETR1PByktHXv6ktQQQ1+SGmLoS1JDdhv6SQ5L8rkkNyfZkuQ1ffvjkmxMcnt/vbJvT5L3JNma5IYkx4y81un98rcnOX3pVkuSNJ+F9PQfBF5fVWuB44CzkqwFzgGurKo1wJX9fYCXAGv6y3rgfOj+SQDnAc8GjgXOm/tHIUmajN2GflXdU1XX9re/B9wCHAKcAlzUL3YR8LL+9inAR6pzNXBgkicALwY2VtX9VfVtYCNw0ljXRpK0S3s0pp9kNXA08BXg4Kq6p3/om8DB/e1DgLtHnratb9tZuyRpQhYc+kkeA3wceG1V/fXoY1VVQI2joCTrk2xOsnnHjh3jeElJUm9BoZ9kX7rA/1hVfaJvvrcftqG/vq9v3w4cNvL0Q/u2nbX/hKq6oKrWVdW6VatW7cm6SJJ2YyF77wT4EHBLVb1z5KENwNweOKcDnxppf1W/F89xwHf7YaArgBOTrOw34J7Yt0mSJmQh0zA8BzgNuDHJ9X3bG4C3AZcmOQO4C3hF/9jlwMnAVuD7wKsBqur+JG8GNvXLvamq7h/LWkjLiNNMaEi7Df2qugrITh4+YZ7lCzhrJ691IXDhnhQoSRofj8iVpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqyEKmVpakBXPq6OlmT1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SG7Db0k1yY5L4kN420vTHJ9iTX95eTRx47N8nWJLclefFI+0l929Yk54x/VSRJu7OQnv6HgZPmaX9XVR3VXy4HSLIWeCXwtP4570+yIskK4H3AS4C1wKn9spKkCdrtmbOq6gtJVi/w9U4BLqmqB4CvJ9kKHNs/trWq7gBIckm/7M17XLEkadH2Zkz/7CQ39MM/K/u2Q4C7R5bZ1rftrP2nJFmfZHOSzTt27NiL8iRJD7fY0D8fOBI4CrgH+INxFVRVF1TVuqpat2rVqnG9rCSJRZ4Yvarunbud5APAZf3d7cBhI4se2rexi3ZJ0oQsqqef5Akjd18OzO3ZswF4ZZJHJTkCWAN8FdgErElyRJJH0m3s3bD4siVJi7Hbnn6Si4EXAAcl2QacB7wgyVFAAXcCZwJU1ZYkl9JtoH0QOKuqfty/ztnAFcAK4MKq2jL2tZEk7dJC9t45dZ7mD+1i+bcAb5mn/XLg8j2qTpI0Vh6RK0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhqyqCNyJalFF1+3baLvd+rRh479Ne3pS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDVkt6Gf5MIk9yW5aaTtcUk2Jrm9v17ZtyfJe5JsTXJDkmNGnnN6v/ztSU5fmtWRJO3KQnr6HwZOeljbOcCVVbUGuLK/D/ASYE1/WQ+cD90/CeA84NnAscB5c/8oJEmTs9vQr6ovAPc/rPkU4KL+9kXAy0baP1Kdq4EDkzwBeDGwsarur6pvAxv56X8kkqQlttgx/YOr6p7+9jeBg/vbhwB3jyy3rW/bWftPSbI+yeYkm3fs2LHI8iRJ89nrDblVVUCNoZa517ugqtZV1bpVq1aN62UlSSw+9O/th23or+/r27cDh40sd2jftrN2SdIELTb0NwBze+CcDnxqpP1V/V48xwHf7YeBrgBOTLKy34B7Yt8mSZqgfXa3QJKLgRcAByXZRrcXztuAS5OcAdwFvKJf/HLgZGAr8H3g1QBVdX+SNwOb+uXeVFUP3zgsSVpiuw39qjp1Jw+dMM+yBZy1k9e5ELhwj6qTJI2VR+RKUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIXsV+knuTHJjkuuTbO7bHpdkY5Lb++uVfXuSvCfJ1iQ3JDlmHCsgSVq4cfT0X1hVR1XVuv7+OcCVVbUGuLK/D/ASYE1/WQ+cP4b3liTtgaUY3jkFuKi/fRHwspH2j1TnauDAJE9YgveXJO3E3oZ+AX+a5Jok6/u2g6vqnv72N4GD+9uHAHePPHdb3yZJmpB99vL5x1fV9iT/ENiY5NbRB6uqktSevGD/z2M9wOGHH76X5UmSRu1VT7+qtvfX9wGfBI4F7p0btumv7+sX3w4cNvL0Q/u2h7/mBVW1rqrWrVq1am/KkyQ9zKJDP8n+SQ6Yuw2cCNwEbABO7xc7HfhUf3sD8Kp+L57jgO+ODANJkiZgb4Z3DgY+mWTudf5nVX0mySbg0iRnAHcBr+iXvxw4GdgKfB949V68tyRpERYd+lV1B/CMedr/CjhhnvYCzlrs+0mS9p5H5EpSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNWTioZ/kpCS3Jdma5JxJv78ktWyioZ9kBfA+4CXAWuDUJGsnWYMktWzSPf1jga1VdUdV/Qi4BDhlwjVIUrNSVZN7s+RXgJOq6jf6+6cBz66qs0eWWQ+s7+8+FbhtYgXCQcC3Jvh+k+b6LW+u3/I16XV7UlWtmu+BfSZYxIJU1QXABUO8d5LNVbVuiPeeBNdveXP9lq9pWrdJD+9sBw4buX9o3yZJmoBJh/4mYE2SI5I8EnglsGHCNUhSsyY6vFNVDyY5G7gCWAFcWFVbJlnDbgwyrDRBrt/y5votX1OzbhPdkCtJGpZH5EpSQwx9SWqIoS9JDTH0Z1ySI4euQYuX5IQk+w1dhxZnGr9/bsgFkvwlcDXwReCLU7ZH0V5J8nm64yE20a3fF6rqxmGrGp8kZ9Ct0+1D17IUklwE/AJwP/3nB1xVVd8etLAxSfJR4PN037tbh65n3Kbx+2foA0keBTwbeC7wHLrpH26oqpcPWtiY9MdEPAt4AXAm8JiqetygRY1Jkt+l+9xWA9fQheIXq+r6IesatyRPBH4F+G3giVU1dUfTL0aSF9J9fs8FjgSuowvGdw9a2BhN2/fP0AeS7EP3oTwfOB54PF3onzloYWOQ5Hge+lIdCFxPF4oXD1rYmPVDIP+GLhQPqaoVA5c0Fkn+Fd1n93S6uVuuovv8vjxoYWPUz777LOCFwL8FflBVPztsVeMxjd8/Qx9I8n3gRuCdwJ9V1V8NXNLYJHmQrgf8VuDyfnbTmZHkP9P9OnsMXS9xLhTvGbSwMUnyLeAvgf8OfK6q7hy2ovFKciWwP/BluuGPq6rqvmGrGp9p/P4Z+kCSU+h6+McCPwK+RPcT88pBCxuDJAfSheLz6HpTfwd8uar+y6CFjUmSa4EHgU/TjQ1/uaoeGLaq8UryNLrP73hgDXBbVZ02bFXjkeRdwDOBB4C/oBue+3JV/WDQwsZkGr9/MzEuuLeq6lPAp5L8LN0JXl4L/A6w7PeaqKrvJLmDbqK7Q4F/Auw7bFXjU1XHJHks3RfrRcAFSe6rquMHLm0s+nU7HHgS3XaLf0AXHDOhql4HkOQA4F8Dfwj8I+BRA5Y1NtP4/bOnDyT5OPAMup/RX6AbIvhKVf1w0MLGoP+Du5V+zyTgq9PwE3Nckvwc3Xjp84F1wN10wzv/ddDCxiTJDXR/j1fR/frcNnBJY9XPxfVcut7+nTy0B91nh6xrXKbx+2foA0nWAddV1Y938viLqmrjhMsaiySPqKqd9gyTnFtVb51kTeOU5DIe+ke9qar+duCSJirJe6vqt4auY7GS/DZdGF5TVQ/O8/jK5bx76jR+/wz9BUhybVUdM3QdS2GW1w26X3FV9c+HrmOpNPD5uX5j5hG5C5OhC1hCs7xuAE8eugDtlVn/+5z4+hn6CzPLP4dmed1g9tdv1s365zfx9TP0Nes9qVnn57e82dOfUncOXcAS+l9DF7DEZj0UZ2a6gp2Y9c9v4t+/pjfkJvnFqvpskl+e7/Gq+sSkaxqXJO9lFz8dq+rfT7CcwSQ5sar+dOg69lSS/82uP79/NsFylkySP3r4gWajbUkeV1X3D1Pd4k3z96/1g7OeD3wW+KfzPFbAsg19YHN//RxgLfDH/f1/Adw8SEVLIMkaukPc1wI/M9deVU/ur5dd4Pfe0V//Mt3BSh/t758K3DtIRUvjaaN3+nl4njl3fzkGfm/z7hcZRtM9/RYkuRo4fm4f6CT70h38ctywlY1HkquA84B30f3zfjXwiBk6OGtzVa3bXdtyk+Rc4A10R71/f66ZbhqUC6rq3KFqm3WO6QNJXpPksel8MMm1SU4cuq4xWQk8duT+Y/q2WbFfP0dSququqnoj8NKBaxqn/ZP8/W6nSY6gm6BsWauqt1bVAcDbq+qx/eWAqnr8LAV+klVJ3pHk8iSfnbsMWVPrwztzfr2q3p3kxXTTKp8G/BGwXIcGRr0NuC7J5+h6Us8D3jhoReP1QJJHALf3h/Rvp/vHNiteB/x5fzh/6ObgWfZTfo+4LMn+VfU3/TTSxwDvrqq7hi5sTD5GN7T6Urppo08HdgxZkMM7dPObVNXPJ3k38OdV9ckk11XV0UPXNg79CThOA24BHg18o6q+MGxV45HkWXTrdSDwZrpfNb9fVV8ZtLAx6k/yMze//K2zNItoP7fQM4CfBz4MfBB4RVU9f8i6xiXJNVX1zLmM6ds2VdWzhqqp+Z5+kgDfSnIF3dGb5/Yz/s3ETIZJfgN4Dd0Mf9cDx9HNXf6LQ9Y1RkX3q+xJPDR74QfoQmTZS3IW8LGq+lp/f2WSM6rq/QOXNi4PVlX105v/t6r6UH8KzFkxNxfUPUleCnwDGPSsdfb0gSQ3A78GfL2fCvXxdGdfumHg0vZakhvp5vG+uqqO6qeP/r2qmnc31eUmyW3Af6Q7Cc7f/6OeleGBJNdX1VEPa5ulX6GfBz5DtwH+ecB9wNeq6umDFjYmSX6JbkK5w4D30v0S/d2q2jBUTc339HubgH2q6jsA/ZmzZuXsWT+sqh8mIcmjqurWJE8duqgx2jHkF2gCViRJ9b2zfpfGRw5c0zj9KvAvgTOq6ptJDgfePnBNY1NVl/U3v0t3OsjB2dMHktwK/GPgLuBv6DaY1dwY3HKW5JN0vajX0g3pfBvYt6pOHrSwMUlyAt2+61fSnX0JWN4H1o1K8na6oav/0TedCdxdVa8friotVJJVdOduXs1IJ7uqfn2wmgx9SPKk+dpnZYhgTpLn05156TNDn8hhXJJ8lG4j5xYeGt6pIb9U49TvmXQmcELftBH44M7O/bBcJLmqqo5P8j1+8sjVuQ7XY3fy1GUlyZfozxcA/P1nVlUfH6wmQ1/LWZLbqmqWhqs0Q+bbJjM0D87ScvelJGuHLmKpJFmT5E+S3JzkjrnL0HWNU5IVSZ6Y5PC5y9A1jdFlSaZqKNWevpa1JLcARwJfpxvTn5ntMdDENBO/Rbd+9/KTw3Oz8vl9j+4I6gfodt8cfPjK0NeyNuvbY0YO7rlxbjfGubahaxuHJFuBZ/d7zGkC3GVTy9qshPsuzPo0E3fT7c44k5LMd/7b7wJ3zXci+Emwpy9NsVmdZiLJf+hvPg14KvBpfnKX23cOUde49bPcHkN38CDA04Gb6Pai+3dDTP1tT1+abrM6zcQB/fX/7S+PZLYOOpvzDboDz7YA9DsdvAn4HbrzdUw89O3pS1Ns1qeZmHVJbqqqn5uvbajdOe3pS9NtJqeZaOV0kMCWJOcDl/T3fxW4uZ859W93/rSlY09fmmKzOs1Ef3Q47OR0kFX1ukEKG7Mk+wG/CRzfN/0F8H7gh8Cjq+r/TbwmQ1+aXg1MMzGTp4OcZg7vSNPtWTM+zcT+SZ5cVXfA7JwOMsmlVfWKfmrzn+pZD3nwmaEvTbcvJVlbVTcPXcgSmdXTQb6mv/6lQauYh8M70hSb9WkmYOZPB7k/8IOq+rskT6Fbz/9TVYNsxAVDX5pqDUwzMXc6yO/091cCp87K6SCTXAM8F1hJtxF3E/Cjqvq1wWoy9CUNpYHTQV5bVcf0E8vtV1W/P/R0y06tLGlIK5Jk7s4Mng4ySX6B7hzcn+7bVgxYjxtyJQ3qM8AfJxk9HeRnBqxn3F4LnAt8sqq2JHky8LkhC3J4R9JgZvV0kNPM0JekMZvmaSYc3pE0mCRrgLcCa4GfmWuvqicPVtR4vKO/nneaiUEq6tnTlzSYBk4HOXXTTLj3jqQh7VdVV9J1QO+qqjcCLx24pnHav994C0zHNBMO70ga0qyfDnLqpplweEfSYGb1dJCjpm2aCYd3JA1p7nSQG4B1wFPoTgc5E/ppJvarqq9V1deARyf5zUFrsqcvaSizfjrIaZxmwjF9SUOaydNBjliRJNX3rqdhmgl7+pIGM6ung5yT5O10G29Hp5m4u6peP1hNhr6koTRwOsipm2bC0Jc0mCS3zfjpIKeOe+9IGtKXkqwduoilkmRNkj9JcnOSO+YuQ9bkhlxJQzoOuD7JrJ4O8g95aJqJF9JPMzFkQQ7vSBpMA6eDvKaqnpnkxqp6+mjbUDXZ05c0mFkJ912Yumkm7OlL0hKZxmkmDH1JWiJJ1gH/iW5f/X375kG3WRj6krREpnGaCcf0JWnpTN00E/b0JWmJTOM0E/b0JWnpvJpumol9GZlmAhgs9O3pS9ISmcZpJpyGQZKWztRNM2FPX5KWSJJbgCOBqZlmwtCXpCUyjdNMGPqS1BDH9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGvL/AXxoLbg0oWkWAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "Y3jFW2reUyjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test, y_pred)"
      ],
      "metadata": {
        "id": "dk18fxqyUy2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPFfnAtbU0c3",
        "outputId": "8ab7a3de-b33d-45d9-f31c-17bb401bae1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.09      0.00      0.00       512\n",
            "           H       0.94      1.00      0.97     17311\n",
            "           S       0.14      0.00      0.00       512\n",
            "\n",
            "    accuracy                           0.94     18335\n",
            "   macro avg       0.39      0.33      0.33     18335\n",
            "weighted avg       0.90      0.94      0.92     18335\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_pred, index=y_test.index).astype('category').to_pickle('.y_pred.pkl')"
      ],
      "metadata": {
        "id": "QH0yN_8YU1Xv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}